{"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 下載網頁","metadata":{}},{"cell_type":"code","source":"pip install lxml","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\nurl = 'https://wealth.yuanta.com.tw/WMECPortal/Wealth/Fundcenter/FundPage/60021903?'\nhtmlfile = requests.get(url)\nprint(type(htmlfile))\n\n'''\nif htmlfile.status_code == requests.codes.ok:\n    print('取得網頁內容成功')\n    print('網頁內容大小=',len(htmlfile.text))\nelse:\n    print('取得網頁內容失敗')\nprint(htmlfile.text)\n'''\n\n","metadata":{"execution":{"iopub.execute_input":"2023-09-13T15:18:02.901259Z","iopub.status.busy":"2023-09-13T15:18:02.900477Z","iopub.status.idle":"2023-09-13T15:18:04.155895Z","shell.execute_reply":"2023-09-13T15:18:04.154907Z","shell.execute_reply.started":"2023-09-13T15:18:02.901210Z"}},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'requests.models.Response'>\n"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":["\"\\nif htmlfile.status_code == requests.codes.ok:\\n    print('取得網頁內容成功')\\n    print('網頁內容大小=',len(htmlfile.text))\\nelse:\\n    print('取得網頁內容失敗')\\nprint(htmlfile.text)\\n\""]},"metadata":{}}]},{"cell_type":"markdown","source":"# **儲存下載的網頁**","metadata":{}},{"cell_type":"code","source":"import requests\n\nurl = 'https://wealth.yuanta.com.tw/WMECPortal/Wealth/Fundcenter/FundPage/60021903?'\n\ntry:\n    htmlfile = requests.get(url)\n    print('下載成功')\nexcept Exception as err:\n    print('下載網頁失敗:%s'% err)\n    \nfn = 'uni_hor.html'\nwith open(fn,'wb') as file_obj:\n    for diskStorage in htmlfile.iter_content(40960):\n        size=file_obj.write(diskStorage)\n        print(size)\n    print('以%s儲存網頁HTML檔案成功'%fn)","metadata":{"execution":{"iopub.execute_input":"2023-09-15T07:12:35.104063Z","iopub.status.busy":"2023-09-15T07:12:35.103624Z","iopub.status.idle":"2023-09-15T07:12:37.019220Z","shell.execute_reply":"2023-09-15T07:12:37.017990Z","shell.execute_reply.started":"2023-09-15T07:12:35.104029Z"}},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"下載成功\n\n40960\n\n4508\n\n以uni_hor.html儲存網頁HTML檔案成功\n"}]},{"cell_type":"code","source":"import urllib.request\n\n#https://www.ntnu.edu.tw/\n#https://wealth.yuanta.com.tw/WMECPortal/Wealth/Fundcenter/FundPage/60021903?\n\nurl = 'https://www.ntnu.edu.tw/'\nhtmlfile = urllib.request.urlopen(url)\n#print(htmlfile.read().decode('utf-8'))   #將二進位顯示為中文\nprint('版本:',htmlfile.version)\nprint('網址:',htmlfile.geturl())\nprint('下載狀況:',htmlfile.status)\nprint('headers:')\n#for header in htmlfile.getheaders():\n    #print(header)","metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:48:58.137346Z","iopub.status.busy":"2023-09-14T06:48:58.136855Z","iopub.status.idle":"2023-09-14T06:48:59.554702Z","shell.execute_reply":"2023-09-14T06:48:59.553650Z","shell.execute_reply.started":"2023-09-14T06:48:58.137311Z"}},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"版本: 11\n\n網址: https://www.ntnu.edu.tw/\n\n下載狀況: 200\n\nheaders:\n"}]},{"cell_type":"markdown","source":"# 偽裝成瀏覽器擷取資料","metadata":{}},{"cell_type":"code","source":"headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n}\n\nurl = 'http://aaa.24ht.com.tw/style=\"color:rgb(175,0,0)\">'\n#req=urllib.request.Request(url,headers=headers)\nhtmlfile = requests.get(url, headers=headers)\nhtmlfile.encoding = 'utf-8'\nhtmlfile.raise_for_status()\nprint('偽裝成功')","metadata":{"execution":{"iopub.execute_input":"2024-07-05T08:06:57.323574Z","iopub.status.busy":"2024-07-05T08:06:57.323186Z","iopub.status.idle":"2024-07-05T08:06:57.745191Z","shell.execute_reply":"2024-07-05T08:06:57.743911Z","shell.execute_reply.started":"2024-07-05T08:06:57.323544Z"}},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"偽裝成功\n"}]},{"cell_type":"code","source":"import requests\n\nurl = 'https://www.httpbin.org/image/jpeg'\nr = requests.get(url)\nimage=r.content\n\nfn = 'out3_38.jpg'\nwith open(fn,'wb') as fout:\n    fout.write(image)","metadata":{"execution":{"iopub.execute_input":"2023-09-14T06:59:21.194742Z","iopub.status.busy":"2023-09-14T06:59:21.193449Z","iopub.status.idle":"2023-09-14T06:59:21.588430Z","shell.execute_reply":"2023-09-14T06:59:21.587201Z","shell.execute_reply.started":"2023-09-14T06:59:21.194690Z"}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 設置代理IP\n使用免費IP www.xiladaili.com","metadata":{}},{"cell_type":"code","source":"import requests\n\nproxies = {\"http\":'http://203.83.182.86:8000'}  #ip:port\n\nr=requests.get('https://docs.python.org',proxies=proxies)\nif r.status_code == 200:\n    print('代理IP使用成功')","metadata":{"execution":{"iopub.execute_input":"2023-09-14T07:08:54.364785Z","iopub.status.busy":"2023-09-14T07:08:54.364315Z","iopub.status.idle":"2023-09-14T07:08:54.459630Z","shell.execute_reply":"2023-09-14T07:08:54.458423Z","shell.execute_reply.started":"2023-09-14T07:08:54.364748Z"}},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"代理IP使用成功\n"}]},{"cell_type":"markdown","source":"# **匯入網頁表格資料**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nurl = 'https://www.stockq.org/market/currency.php'\naa=pd.read_html(url)\n\nitem=0\nfor a in aa:\n    print('元素:',item)\n    print(a)\n    print()\n    item+=1","metadata":{"execution":{"iopub.execute_input":"2023-09-14T07:15:19.012873Z","iopub.status.busy":"2023-09-14T07:15:19.012117Z","iopub.status.idle":"2023-09-14T07:15:19.303207Z","shell.execute_reply":"2023-09-14T07:15:19.301806Z","shell.execute_reply.started":"2023-09-14T07:15:19.012837Z"}},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"元素: 0\n\n                                                   0\n\n0  (adsbygoogle = window.adsbygoogle || []).push(...\n\n\n\n元素: 1\n\n                                                   0\n\n0  (adsbygoogle = window.adsbygoogle || []).push(...\n\n1  首頁 市場動態 歷史股價 基金淨值 基金分類 經濟數據 公債利率指數 公債ETF  指數期貨...\n\n\n\n元素: 2\n\n    0                                                  1\n\n0 NaN  (adsbygoogle = window.adsbygoogle || []).push(...\n\n\n\n元素: 3\n\n                                                   0                   1\n\n0  首頁 市場動態 歷史股價 基金淨值 基金分類 經濟數據 公債利率指數 公債ETF  指數期貨...  2023/9/14 15:15:19\n\n\n\n元素: 4\n\n                                                   0  \\\n\n0  市場動態 全球股市排行榜 相對低檔股市指數 相對高檔股市指數 國際指數期貨 股市指數歷史資料...   \n\n\n\n                                                   1  \n\n0  全球貨幣匯率 2023/9/14 23:15:19  全球匯率 (Currency Exch...  \n\n\n\n元素: 5\n\n                                                   0\n\n0  市場動態 全球股市排行榜 相對低檔股市指數 相對高檔股市指數 國際指數期貨 股市指數歷史資料...\n\n\n\n元素: 6\n\n                                                   0\n\n0  ETF 台灣ETF列表 指數ETF 正2反1 ETF AI科技 ETF 電動車ETF 高股息...\n\n\n\n元素: 7\n\n                                               0\n\n0  債券ETF 美國政府公債ETF 投資級公司債ETF 非投資級債券ETF 新興市場債券ETF\n\n\n\n元素: 8\n\n                           0   1\n\n0  全球貨幣匯率 2023/9/14 23:15:19 NaN\n\n\n\n元素: 9\n\n                           0                         1  \\\n\n0   全球匯率 (Currency Exchange)  全球匯率 (Currency Exchange)   \n\n1                         貨幣                        匯率   \n\n2                      歐元/美元                    1.0739   \n\n3                      英鎊/美元                    1.2489   \n\n4                    美元/瑞士法郎                    0.8927   \n\n5                    美元/瑞典克朗                   11.1104   \n\n6                   美元/俄羅斯盧布                   96.2596   \n\n7                    美元/烏克蘭幣                   36.6422   \n\n8                    美元/匈牙利幣                    357.21   \n\n9                    美元/土耳其幣                   26.9066   \n\n10                    美元/南非幣                   18.7900   \n\n11                   美元/以色列幣                    3.8133   \n\n12                    美元/摩洛哥                    9.8442   \n\n13                     澳幣/美元                    0.6433   \n\n14                     紐幣/美元                    0.5931   \n\n15                     美元/日圓                    147.12   \n\n16                    美元/人民幣                    7.2744   \n\n17                     美元/港幣                    7.8251   \n\n18                     美元/台幣                    31.904   \n\n19                     美元/韓圜                   1324.74   \n\n20                     美元/泰銖                    35.735   \n\n21                     美元/新元                    1.3599   \n\n22                    美元/菲披索                    56.715   \n\n23                    美元/馬來幣                    4.6790   \n\n24                    美元/印尼盾                   15354.4   \n\n25                   美元/印度盧比                    82.979   \n\n26                     美元/加幣                    1.3539   \n\n27                    美元/巴西幣                    4.9158   \n\n28                  美元/墨西哥披索                   17.1250   \n\n29                    美元/阿根廷                  349.9700   \n\n30                     美元/智利                    879.90   \n\n\n\n                           2                         3  \\\n\n0   全球匯率 (Currency Exchange)  全球匯率 (Currency Exchange)   \n\n1                         漲跌                        比例   \n\n2                     0.0013                     0.12%   \n\n3                     0.0003                     0.02%   \n\n4                    -0.0008                    -0.09%   \n\n5                    -0.0130                    -0.12%   \n\n6                     0.0346                     0.04%   \n\n7                    -0.0203                    -0.06%   \n\n8                      -0.43                    -0.12%   \n\n9                     0.0326                     0.12%   \n\n10                   -0.0095                    -0.05%   \n\n11                   -0.0046                    -0.12%   \n\n12                   -0.0053                    -0.05%   \n\n13                    0.0014                     0.22%   \n\n14                    0.0014                     0.24%   \n\n15                     -0.32                    -0.21%   \n\n16                    0.0044                     0.06%   \n\n17                   -0.0006                    -0.01%   \n\n18                    -0.033                    -0.10%   \n\n19                     -2.68                    -0.20%   \n\n20                     0.022                     0.06%   \n\n21                   -0.0008                    -0.06%   \n\n22                     0.055                     0.10%   \n\n23                    0.0015                     0.03%   \n\n24                      10.6                     0.07%   \n\n25                    -0.021                    -0.03%   \n\n26                   -0.0005                    -0.04%   \n\n27                    0.0010                     0.02%   \n\n28                   -0.0165                    -0.10%   \n\n29                   -0.0522                    -0.01%   \n\n30                    -11.60                    -1.30%   \n\n\n\n                           4  \n\n0   全球匯率 (Currency Exchange)  \n\n1                         台北  \n\n2                      15:06  \n\n3                      15:07  \n\n4                      15:06  \n\n5                      15:06  \n\n6                      15:07  \n\n7                      15:07  \n\n8                      15:06  \n\n9                      15:06  \n\n10                     15:06  \n\n11                     15:06  \n\n12                     15:06  \n\n13                     15:06  \n\n14                     15:07  \n\n15                     15:06  \n\n16                     15:07  \n\n17                     15:06  \n\n18                     15:05  \n\n19                     15:06  \n\n20                     15:06  \n\n21                     15:06  \n\n22                     15:06  \n\n23                     15:05  \n\n24                     15:06  \n\n25                     15:06  \n\n26                     15:07  \n\n27                     09/13  \n\n28                     15:06  \n\n29                     15:06  \n\n30                     15:00  \n\n\n\n元素: 10\n\n                                                   0\n\n0  (adsbygoogle = window.adsbygoogle || []).push(...\n\n\n\n元素: 11\n\n                                                   0\n\n0  統一發票  | 12星座分析  | 行動網站  | Wordle每日答案  | Wordle...\n\n1  全球股市指數 / 国际股市指数 / World Indices / Мировые инде...\n\n2                         紅漲綠跌  綠漲紅跌  回復預設(刪除cookie)\n\n\n"}]},{"cell_type":"code","source":"import pandas as pd\n\nurl = 'https://www.stockq.org/market/currency.php'\naa=pd.read_html(url)\n\na=aa[9]\na=a.drop(a.index[[0,1]])\na.columns=['貨幣','匯率','漲跌','比例','台北']\nprint(a)","metadata":{"execution":{"iopub.execute_input":"2023-09-14T07:23:43.358783Z","iopub.status.busy":"2023-09-14T07:23:43.357873Z","iopub.status.idle":"2023-09-14T07:23:43.542149Z","shell.execute_reply":"2023-09-14T07:23:43.540764Z","shell.execute_reply.started":"2023-09-14T07:23:43.358726Z"}},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"          貨幣        匯率       漲跌      比例     台北\n\n2      歐元/美元    1.0741   0.0014   0.14%  14:55\n\n3      英鎊/美元    1.2493   0.0006   0.05%  14:55\n\n4    美元/瑞士法郎    0.8919  -0.0015  -0.17%  14:55\n\n5    美元/瑞典克朗   11.1170  -0.0064  -0.06%  14:56\n\n6   美元/俄羅斯盧布   96.5265   0.3015   0.31%  14:55\n\n7    美元/烏克蘭幣   36.6422  -0.0203  -0.06%  14:54\n\n8    美元/匈牙利幣    357.27    -0.37  -0.10%  14:54\n\n9    美元/土耳其幣   26.9467   0.0293   0.11%  14:55\n\n10    美元/南非幣   18.7927   0.0006   0.00%  14:56\n\n11   美元/以色列幣    3.8159  -0.0022  -0.06%  14:55\n\n12    美元/摩洛哥    9.8442  -0.0053  -0.05%  14:54\n\n13     澳幣/美元    0.6438   0.0018   0.28%  14:55\n\n14     紐幣/美元    0.5933   0.0019   0.31%  14:55\n\n15     美元/日圓    147.10    -0.34  -0.23%  14:55\n\n16    美元/人民幣    7.2742   0.0044   0.06%  14:55\n\n17     美元/港幣    7.8252  -0.0005  -0.01%  14:55\n\n18     美元/台幣    31.906   -0.031  -0.10%  14:56\n\n19     美元/韓圜   1325.65    -2.63  -0.20%  14:55\n\n20     美元/泰銖    35.729    0.016   0.04%  14:54\n\n21     美元/新元    1.3596  -0.0010  -0.08%  14:56\n\n22    美元/菲披索    56.710    0.049   0.09%  14:54\n\n23    美元/馬來幣    4.6790   0.0015   0.03%  14:55\n\n24    美元/印尼盾   15353.6      9.9   0.06%  14:54\n\n25   美元/印度盧比    82.969   -0.031  -0.04%  14:55\n\n26     美元/加幣    1.3535  -0.0009  -0.07%  14:55\n\n27    美元/巴西幣    4.9158   0.0010   0.02%  09/13\n\n28  美元/墨西哥披索   17.1257  -0.0160  -0.09%  14:55\n\n29    美元/阿根廷  349.9700  -0.0522  -0.01%  14:54\n\n30     美元/智利    879.90   -11.60  -1.30%  14:30\n"}]},{"cell_type":"markdown","source":"# **BeautifulSoup**","metadata":{}},{"cell_type":"code","source":"pip install beautifulsoup4","metadata":{"execution":{"iopub.execute_input":"2023-09-20T12:19:57.825835Z","iopub.status.busy":"2023-09-20T12:19:57.825471Z","iopub.status.idle":"2023-09-20T12:20:13.722284Z","shell.execute_reply":"2023-09-20T12:20:13.720798Z","shell.execute_reply.started":"2023-09-20T12:19:57.825806Z"}},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.2)\n\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"import bs4\n\nurl = 'http://www.grandtech.info/'\nr = requests.get(url)\nobjSoup=bs4.BeautifulSoup(r.text,'lxml')\nprint(type(objSoup))","metadata":{"execution":{"iopub.execute_input":"2023-09-15T07:13:49.505939Z","iopub.status.busy":"2023-09-15T07:13:49.505430Z","iopub.status.idle":"2023-09-15T07:13:49.958662Z","shell.execute_reply":"2023-09-15T07:13:49.957678Z","shell.execute_reply.started":"2023-09-15T07:13:49.505882Z"}},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'bs4.BeautifulSoup'>\n"}]},{"cell_type":"code","source":"'''\nimport bs4\n\nhtml=open('uni_hor.html',encoding='utf-8')\nobjSoup=bs4.BeautifulSoup(html,'lxml')\ndataTag=objSoup.select('.ywm_fi_cell')\nprint('串列長度',len(dataTag))\n#print('列印title = ',objSoup.title.text)\nfor i in range(len(dataTag)):\n    print(dataTag[i])\n\nprice=dataTag[1].find_all('h4',{'class':'red'})\nprint('淨值＝',price)\n\n\n#print('列印Tag.string = ',objTag)\n'''","metadata":{"execution":{"iopub.execute_input":"2023-09-15T07:37:41.378813Z","iopub.status.busy":"2023-09-15T07:37:41.378320Z","iopub.status.idle":"2023-09-15T07:37:41.423182Z","shell.execute_reply":"2023-09-15T07:37:41.421992Z","shell.execute_reply.started":"2023-09-15T07:37:41.378776Z"}},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"串列長度 4\n\n<div class=\"ywm_fi_cell\">\n\n<h4>統一黑馬基金</h4>\n\n<div class=\"ywm_fi_sec\">WMF001431</div>\n\n<div class=\"ywm_fi_sec\">台灣股票</div>\n\n</div>\n\n<div class=\"ywm_fi_cell\">\n\n<h4 class=\"red\">189.12<span>TWD</span></h4>\n\n<div class=\"ywm_fi_sec\">2023/09/14</div>\n\n</div>\n\n<div class=\"ywm_fi_cell\">\n\n<ul>\n\n<li>52周最高<span>203.7</span></li>\n\n<li>52周最低<span>104.65</span></li>\n\n</ul>\n\n<ul>\n\n<li>日漲跌<span class=\"red\">4.06</span></li>\n\n<li>漲跌幅<span class=\"red\">2.19 %</span></li>\n\n</ul>\n\n</div>\n\n<div class=\"ywm_fi_cell\">\n\n<div class=\"ywm_fi_btn\">\n\n<a class=\"ywm_fib01\" href=\"https://wealth.yuanta.com.tw/EC1200/EC120201.aspx?prodId=WMF001431\">單筆申購</a>\n\n<a class=\"ywm_fib02\" href=\"https://wealth.yuanta.com.tw/EC1200/EC120202.aspx?prodId=WMF001431\">定期定額</a>\n\n<a class=\"ywm_fib03\" href=\"https://wealth.yuanta.com.tw/EC1200/EC120203.aspx\">贖回</a>\n\n<a class=\"ywm_fib04\" href=\"https://wealth.yuanta.com.tw/EC1200/EC120206.aspx\">轉換</a>\n\n<a class=\"ywm_fib05\" href=\"javascript:AddtoObserved('WMF001431');\">加入觀察</a>\n\n</div>\n\n</div>\n\n淨值＝ [<h4 class=\"red\">189.12<span>TWD</span></h4>]\n"}]},{"cell_type":"code","source":"#pip install beautifulsoup4","metadata":{"execution":{"iopub.execute_input":"2024-07-05T07:58:25.414022Z","iopub.status.busy":"2024-07-05T07:58:25.413435Z","iopub.status.idle":"2024-07-05T07:58:38.692215Z","shell.execute_reply":"2024-07-05T07:58:38.690835Z","shell.execute_reply.started":"2024-07-05T07:58:25.413984Z"}},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.2)\n\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"code","source":"import bs4\nimport requests\n\n\nurl = 'https://wealth.yuanta.com.tw/WMECPortal/Wealth/Fundcenter/FundPage/60021903?'\nhtml = requests.get(url)\nhtml.raise_for_status()\nprint('網頁下載成功')\n\n#html = open('/kaggle/working/uni_hor.html', encoding='utf-8')\nobjSoup = bs4.BeautifulSoup(html.text, 'lxml')\ndateTag=objSoup.select('.ywm_fi_sec')\n#print('串列長度',len(dateTag))\n\n#for i in range(len(dateTag)):\n    #print(dateTag[i])\n\ndate_element = dateTag[2]\ndate_text = date_element.text  # 获取包含价格和\" TWD\" 的完整文本\nprint('收盤日期為', date_text)\n\n    \ndataTag = objSoup.select('.ywm_fi_cell')\n#print('串列長度', len(dataTag))\n\n\nprice_element = dataTag[1].find('h4', {'class': 'red'})\nprice_text = price_element.text  # 获取包含价格和\" TWD\" 的完整文本\nprice_parts = price_text.split()  # 将文本拆分成单词列表\nnumeric_part = price_parts[0]  # 获取第一个单词（数字部分）\nprice = numeric_part  # 将数字部分转换为浮点数\nprint('淨值＝', price)\n","metadata":{"execution":{"iopub.execute_input":"2024-07-05T07:24:24.408151Z","iopub.status.busy":"2024-07-05T07:24:24.407769Z","iopub.status.idle":"2024-07-05T07:24:26.433800Z","shell.execute_reply":"2024-07-05T07:24:26.432361Z","shell.execute_reply.started":"2024-07-05T07:24:24.408121Z"}},"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"網頁下載成功\n\n收盤日期為 2024/07/04\n\n淨值＝ 232.1TWD\n"}]},{"cell_type":"code","source":"pip install PyAutoGUI","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting PyAutoGUI\n\n  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n\n     -------------------------------------- 61.2/61.2 kB 542.5 kB/s eta 0:00:00\n\n  Installing build dependencies: started\n\n  Installing build dependencies: finished with status 'done'\n\n  Getting requirements to build wheel: started\n\n  Getting requirements to build wheel: finished with status 'done'\n\n  Preparing metadata (pyproject.toml): started\n\n  Preparing metadata (pyproject.toml): finished with status 'done'\n\nCollecting pyscreeze>=0.1.21\n\n  Downloading PyScreeze-0.1.30.tar.gz (27 kB)\n\n  Installing build dependencies: started\n\n  Installing build dependencies: finished with status 'done'\n\n  Getting requirements to build wheel: started\n\n  Getting requirements to build wheel: finished with status 'done'\n\n  Preparing metadata (pyproject.toml): started\n\n  Preparing metadata (pyproject.toml): finished with status 'done'\n\nCollecting pymsgbox\n\n  Downloading PyMsgBox-1.0.9.tar.gz (18 kB)\n\n  Installing build dependencies: started\n\n  Installing build dependencies: finished with status 'done'\n\n  Getting requirements to build wheel: started\n\n  Getting requirements to build wheel: finished with status 'done'\n\n  Preparing metadata (pyproject.toml): started\n\n  Preparing metadata (pyproject.toml): finished with status 'done'\n\nCollecting pytweening>=1.0.4\n\n  Downloading pytweening-1.2.0.tar.gz (171 kB)\n\n     -------------------------------------- 171.2/171.2 kB 2.6 MB/s eta 0:00:00\n\n  Preparing metadata (setup.py): started\n\n  Preparing metadata (setup.py): finished with status 'done'\n\nCollecting mouseinfo\n\n  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n\n  Preparing metadata (setup.py): started\n\n  Preparing metadata (setup.py): finished with status 'done'\n\nCollecting pygetwindow>=0.0.5\n\n  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n\n  Preparing metadata (setup.py): started\n\n  Preparing metadata (setup.py): finished with status 'done'\n\nCollecting pyrect\n\n  Downloading PyRect-0.2.0.tar.gz (17 kB)\n\n  Preparing metadata (setup.py): started\n\n  Preparing metadata (setup.py): finished with status 'done'\n\nRequirement already satisfied: Pillow>=9.2.0 in c:\\users\\dominic\\anaconda3\\lib\\site-packages (from pyscreeze>=0.1.21->PyAutoGUI) (9.2.0)\n\nCollecting pyperclip\n\n  Downloading pyperclip-1.9.0.tar.gz (20 kB)\n\n  Preparing metadata (setup.py): started\n\n  Preparing metadata (setup.py): finished with status 'done'\n\nBuilding wheels for collected packages: PyAutoGUI, pygetwindow, pyscreeze, pytweening, mouseinfo, pymsgbox, pyperclip, pyrect\n\n  Building wheel for PyAutoGUI (pyproject.toml): started\n\n  Building wheel for PyAutoGUI (pyproject.toml): finished with status 'done'\n\n  Created wheel for PyAutoGUI: filename=PyAutoGUI-0.9.54-py3-none-any.whl size=37597 sha256=ae860206a3704730df2d160ad1404ebeebb073f9d8c8e5317feadb3ca58f1cd0\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\29\\2b\\5f\\9df8525a15d14d1a3ba04fe66e5bd3d2dc8ca7f5fb2ab2ec50\n\n  Building wheel for pygetwindow (setup.py): started\n\n  Building wheel for pygetwindow (setup.py): finished with status 'done'\n\n  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11063 sha256=44d4c22233313d97a24278f5e1d01210510537aa7497ec730647227c6f7ba41b\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\44\\ab\\20\\423c3a444793767e4e41f8377bc902f77bee212e68dcce85a5\n\n  Building wheel for pyscreeze (pyproject.toml): started\n\n  Building wheel for pyscreeze (pyproject.toml): finished with status 'done'\n\n  Created wheel for pyscreeze: filename=PyScreeze-0.1.30-py3-none-any.whl size=14399 sha256=307d82b031e417dde0e216fbfe0158f7db3e0cc9b73ddd38078baf56677c56ad\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\57\\ef\\26\\25869f8f3fbfc108d6d5a50f57184291825caedc8ab1eaa1d6\n\n  Building wheel for pytweening (setup.py): started\n\n  Building wheel for pytweening (setup.py): finished with status 'done'\n\n  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8010 sha256=00d751ad698f1ba7fe7371c0e6bc88d8c0f34b14a749da13068e3d1ea095833e\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\eb\\48\\3d\\ded50b5e7d57738e4bd3ee314e537f4f646328aeaef51d2619\n\n  Building wheel for mouseinfo (setup.py): started\n\n  Building wheel for mouseinfo (setup.py): finished with status 'done'\n\n  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10891 sha256=5cc6c05284e4b44415a362183fad4fa1d49d5907160de0495f86a0647064e667\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\61\\73\\b9\\6fb1131ab36e650206e3aa0ad7a68907b41b32ac2d4f75f543\n\n  Building wheel for pymsgbox (pyproject.toml): started\n\n  Building wheel for pymsgbox (pyproject.toml): finished with status 'done'\n\n  Created wheel for pymsgbox: filename=PyMsgBox-1.0.9-py3-none-any.whl size=7416 sha256=10c1694229369538a6cc0a56929b041d6b885eed9ef61b3c8069c8617248a184\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\7f\\13\\8c\\584c519464297d9637f9cd29fd1dcdf55e2a2cab225c76a2db\n\n  Building wheel for pyperclip (setup.py): started\n\n  Building wheel for pyperclip (setup.py): finished with status 'done'\n\n  Created wheel for pyperclip: filename=pyperclip-1.9.0-py3-none-any.whl size=11002 sha256=8c3887ab85372e708e98b9ec1c7b7fd63fe93a1a897c4ae186f5b8fb41973ba3\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\b2\\12\\87\\f136269baa23c0afc652d05f3bd48834f4f6658b42365036c7\n\n  Building wheel for pyrect (setup.py): started\n\n  Building wheel for pyrect (setup.py): finished with status 'done'\n\n  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11179 sha256=e97a3173d46e82684f9e14bfd1af2babd272ce1d623c55647972937879e4c7c5\n\n  Stored in directory: c:\\users\\dominic\\appdata\\local\\pip\\cache\\wheels\\25\\80\\fa\\27bb4a1c2e21f64ec71390489d52e57b7cc8afbe79bd595c5e\n\nSuccessfully built PyAutoGUI pygetwindow pyscreeze pytweening mouseinfo pymsgbox pyperclip pyrect\n\nInstalling collected packages: pytweening, pyrect, pyperclip, pymsgbox, pyscreeze, pygetwindow, mouseinfo, PyAutoGUI\n\nSuccessfully installed PyAutoGUI-0.9.54 mouseinfo-0.1.3 pygetwindow-0.0.9 pymsgbox-1.0.9 pyperclip-1.9.0 pyrect-0.2.0 pyscreeze-0.1.30 pytweening-1.2.0\n\nNote: you may need to restart the kernel to use updated packages.\n"}]},{"cell_type":"markdown","source":"uTRd8WZAkORjjq5AYTwWqSCx7MuKMrzX9N9FiqkehsX","metadata":{}},{"cell_type":"code","source":"import requests\n\ndef send_message(msg):\n    headers = {\n        'Authorization': 'Bearer uTRd8WZAkORjjq5AYTwWqSCx7MuKMrzX9N9FiqkehsX'\n    }\n\n    payload = {\n        'message': msg\n    }\n    #files = {'imageFile': open(image_path, 'rb')}\n\n    r = requests.post(\"https://notify-api.line.me/api/notify\", headers=headers, data=payload)\n    return r.status_code, r.json()\n","metadata":{"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 訊息和圖片路徑\n#msg = 'test'\n#imgfile = r'C:/Users/Dominic/Desktop/小說集/家裡蹲妹妹竟然要當冒險者/03447-3233128412-blue cloak.png'\n\n#status_code, response_data = send_message(msg)\n#print(f\"狀態碼: {status_code}\")","metadata":{"tags":[]},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"狀態碼: 200\n"}]},{"cell_type":"markdown","source":"透過取得世芯-KY、奇鋐及台積電的每日開盤、收盤、成交量，決定黑馬將是大紅或大綠。\n\n之後可以建立機器學習模型，配對什麼情況下的黑馬是可進場的。搭配line機器人，回報可否進場。","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport time\n\nsymboldict={'3661':'世芯-KY','3017':'奇鋐','2330':'台積電'}\n\n#民國轉西元年函數\ndef transform_date(date):\n    parts = date.split('/') \n    y, m, d = parts\n    return str(int(y) + 1911) + '/' + m + '/' + d","metadata":{"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_data(begin,stocks):\n    print('start...')\n    \n    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n    }\n    \n    #目標網址\n    baseurl='https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY?date={}&stockNo={}&response=html'.format(begin,stocks)\n    \n    #加上content可以解決某些回傳失敗問題\n    data=requests.get(url=baseurl,headers=headers).content\n    #title=BeautifulSoup(data,'html.parser').find('thead').find('tr')\n    soup = BeautifulSoup(data, 'html.parser')\n    title = soup.find('thead')\n    if title is None:\n        raise ValueError(f\"Could not find table header (thead) in the data for {stocks} on {begin}\")\n    \n    title_row = title.find('tr')\n    if title_row is None:\n        raise ValueError(f\"Could not find table row (tr) in the table header for {stocks} on {begin}\")\n    \n    \n    datalist=[]\n    for col in title_row.find_all_next('tr'):\n        datalist.append([row.text for row in col.find_all('td')])\n        \n    \n    #刪除第一行不需要的數據\n    for each in datalist[1:]:\n        each[0]=transform_date(each[0])\n    \n    #print(datalist[1:])\n    #print(datalist[1:2])\n    df=pd.DataFrame(datalist[1:],columns=datalist[1])\n    df.columns=datalist[1]\n    \n    print('{}{}_{}資料搜集成功'.format(stocks,symboldict[stocks],begin))\n    \n    return df\n    ","metadata":{"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def data_to_csv(input_dataframe,stocks):\n    directory = 'uniblack_stocks'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    #確認該股票檔案是否存在\n    if os.path.isfile('{}{}.csv'.format(stocks,symboldict[stocks])):\n        #用異常處理讀取檔案，藉此檢查是否有問題 \n        try:\n            cu_data=pd.read_csv('{}{}.csv'.format(stocks,symboldict[stocks]))\n            \n            if input_dataframe['日期'][0] in list(cu_data['日期']):\n                print('資料檢查結果：有重複日期')\n                print('不寫入')\n                time.sleep(1)\n            else:\n                print('資料檢查結果：無重複資料...寫入中...')\n                filepath = os.path.join(directory, '{}{}.csv'.format(stocks, symboldict[stocks]))\n                input_dataframe.to_csv(filepath,mode='a',header=False)\n                #input_dataframe.to_csv('{}{}.csv'.format(stocks,symboldict[stocks]),mode='a',header=False)\n                print('寫入完成！')\n                time.sleep(1)\n        except:\n            print('某步驟錯誤')\n    else:\n        print('創建新資料...')\n        filepath = os.path.join(directory, '{}{}.csv'.format(stocks, symboldict[stocks]))\n        input_dataframe.to_csv(filepath, mode='a',header=False)\n        #input_dataframe.to_csv('{}{}.csv'.format(stocks,symboldict[stocks]),mode='w')\n        \n        print('寫入完成！')\n        time.sleep(1)","metadata":{"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def diff_datetime(start_year, start_month, end_year, end_month):\n    year_list = []\n\n    for i in range(end_year - start_year + 1):\n        year_list.append(start_year + i)\n\n    whole_date = []\n    for strtime in year_list:\n        if strtime == start_year and strtime != end_year:\n            #aa='進入那裡'\n            for mon in range(start_month, 13):\n                if mon > 9:\n                    str_sm = mon\n                    whole_date.append('{}{}01'.format(strtime, str_sm))\n                elif mon <= 9:\n                    str_sm = '0{}'.format(mon)\n                    whole_date.append('{}{}01'.format(strtime, str_sm))\n                else:\n                    print('請輸入1-12')\n        elif strtime == end_year and start_month != end_month:\n            #aa='進入這裡'\n            for mon in range(1, end_month + 1):\n                if mon > 9:\n                    end_sm = mon\n                    whole_date.append('{}{}01'.format(strtime, end_sm))\n                elif mon <= 9:\n                    end_sm = '0{}'.format(mon)\n                    whole_date.append('{}{}01'.format(strtime, end_sm))\n                else:\n                    print('請輸入1-12')\n        else:\n            for nor_mon in range(start_month, end_month+1):\n                #aa='進入nor_mon'\n                if nor_mon > 9:\n                    nor_m = nor_mon\n                    whole_date.append('{}{}01'.format(strtime, nor_m))\n                elif nor_mon <= 9:\n                    whole_date.append('{}0{}01'.format(strtime, nor_mon))\n                else:\n                    print('請輸入1-12')\n\n    return whole_date\n","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport os\n\ncode=['3661','3017','2330']\ncralwer_date=diff_datetime(2023,1,2024,7)\n#print(cralwer_date)\n\nfor sn in code:\n    for dn in cralwer_date:\n        data_to_csv(get_data(dn,sn),sn)\n        time.sleep(3)\n","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":48,"outputs":[{"name":"stdout","output_type":"stream","text":"start...\n\n3661世芯-KY_20230101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230801資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20230901資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20231001資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20231101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20231201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3661世芯-KY_20240701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230801資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20230901資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20231001資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20231101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20231201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n3017奇鋐_20240701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230801資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20230901資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20231001資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20231101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20231201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240101資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240201資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240301資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240401資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240501資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240601資料搜集成功\n\n創建新資料...\n\n寫入完成！\n\nstart...\n\n2330台積電_20240701資料搜集成功\n\n創建新資料...\n\n寫入完成！\n"}]},{"cell_type":"code","source":"print(diff_datetime(2024,7,2024,7))","metadata":{},"execution_count":22,"outputs":[{"name":"stdout","output_type":"stream","text":"['20240701']\n"}]},{"cell_type":"markdown","source":"curl -X POST https://notify-api.line.me/api/notify -H \"Authorization: Bearer uTRd8WZAkORjjq5AYTwWqSCx7MuKMrzX9N9FiqkehsX\" -F \"message=test\" -F \"imageFile=@Desktop/uniblack_ana/hist_fund_predict.png\"\n","metadata":{}},{"cell_type":"code","source":"import requests\n\nurl = 'https://fund.taipeifubon.com.tw/w/wr/wr02_ACPS02-0603.djhtm'\nheaders={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n}\ntry:\n    htmlfile = requests.get(url, headers=headers)\n    print('下載成功')\nexcept Exception as err:\n    print('下載網頁失敗:%s'% err)\n    \nfn = 'uni_hor.html'\nwith open(fn,'wb') as file_obj:\n    for diskStorage in htmlfile.iter_content(40960):\n        size=file_obj.write(diskStorage)\n        print(size)\n    print('以%s儲存網頁HTML檔案成功'%fn)","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"下載成功\n\n16799\n\n以uniblack_stocks/uni_hor.html儲存網頁HTML檔案成功\n"}]},{"cell_type":"code","source":"pip install tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install chardet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bs4\nfrom tqdm import tqdm\nimport chardet\n\nwith open('uni_hor.html', 'rb') as f:\n    raw_data = f.read()\n    result = chardet.detect(raw_data)\n    encoding = result['encoding']\n    print(f\"Detected encoding: {encoding}\")\n\n# 使用檢測到的編碼讀取文件\nwith open('uni_hor.html', encoding=encoding) as html_file:\n    html_content = html_file.read()\n\n\n# 使用 BeautifulSoup 解析 HTML 文件的內容\nobjSoup = bs4.BeautifulSoup(html_content, 'lxml')\n\n# 將解析後的結果轉換為字符串並按行分割\npretty_html = objSoup.prettify().split('\\n')\n\n# 使用 tqdm 顯示進度條逐行打印解析後的結果\nfor line in tqdm(pretty_html, desc=\"Printing HTML\"):\n    print(line)","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Detected encoding: Big5\n"},{"name":"stderr","output_type":"stream","text":"Printing HTML: 100%|██████████| 810/810 [00:00<00:00, 269955.20it/s]"},{"name":"stdout","output_type":"stream","text":"<!DOCTYPE >\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n\n <head>\n\n  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n\n  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n\n  <script src=\"/w/js/jquery-latest.djjs\" type=\"text/javascript\">\n\n  </script>\n\n  <script src=\"/w/js/Applet2Canvas.djjs\" type=\"text/javascript\">\n\n  </script>\n\n  <script src=\"/w/js/DJWebGraph.djjs\" type=\"text/javascript\">\n\n  </script>\n\n  <title>\n\n   國內基金淨值表\n\n  </title>\n\n  <link href=\"/w/js/wFund.css\" rel=\"stylesheet\" type=\"text/css\"/>\n\n  <script language=\"JavaScript\" src=\"/w/js/cookie.js\">\n\n  </script>\n\n  <script language=\"JavaScript\" src=\"/w/js/wtfundjs.djjs\">\n\n  </script>\n\n  <script language=\"JavaScript\" src=\"/w/js/wtfund.js\">\n\n  </script>\n\n  <script language=\"JavaScript\" src=\"/w/js/jsFunction.js\">\n\n  </script>\n\n  <script src=\"/w/js/jquery-ui/jquery-ui.min.js\" type=\"text/javascript\">\n\n  </script>\n\n  <link href=\"/w/js/jquery-ui/jquery-ui.css\" rel=\"stylesheet\"/>\n\n  <script language=\"JavaScript\">\n\n   <!--\n\nCheckMenu('wr02','ACPS02','NA','NA');\n\n// -->\n\n  </script>\n\n </head>\n\n <body>\n\n  <script language=\"javascript\" src=\"/w/js/SU.js\">\n\n  </script>\n\n  <center>\n\n   <div id=\"buyFundDialog\" style=\"display: none;\" title=\"申購方式\">\n\n    <p>\n\n    </p>\n\n    請選擇下列申購方式\n\n   </div>\n\n   <div id=\"SysJustIFRAMEDIV\">\n\n    <table border=\"0\" width=\"680\">\n\n     <tr>\n\n      <td class=\"wfb00c\">\n\n       <script language=\"javascript\">\n\n        <!--\n\nwindow.onload = GoStart;\n\n\n\nfunction GoStart()\n\n{\n\n\tComboReset('ACPS02-0603');\n\n}\n\n\n\nfunction FundGoPage(sObj)\n\n{\n\n\tvar sURL = '/w/wr/wr02.djhtm?a=';\n\n\tvar sFID = sObj.selTFund3.options[sObj.selTFund3.selectedIndex].value;\n\n\tif ( sFID != '0' )\n\n\t\tdocument.location = sURL + sFID;\n\n}\n\n//-->\n\n       </script>\n\n       <table border=\"0\" cellpadding=\"1\" cellspacing=\"1\" class=\"wfb0c\" id=\"oMainTable\" style=\"border-collapse: separate\" width=\"680\">\n\n        <form method=\"POST\" name=\"wr02_frm\">\n\n         <div class=\"tabs\">\n\n          <ul>\n\n           <li class=\"\">\n\n            <a href=\"/w/wr/wr01_ACPS02-0603.djhtm\">\n\n             基本資料\n\n            </a>\n\n           </li>\n\n           <li class=\"on\">\n\n            <a href=\"/w/wr/wr02_ACPS02-0603.djhtm\">\n\n             淨值走勢\n\n            </a>\n\n           </li>\n\n           <li class=\"\">\n\n            <a href=\"/w/wr/wr03_ACPS02-0603.djhtm\">\n\n             績效分析\n\n            </a>\n\n           </li>\n\n           <li class=\"\">\n\n            <a href=\"/w/wr/wr04_ACPS02-0603.djhtm\">\n\n             持股狀況\n\n            </a>\n\n           </li>\n\n           <li class=\"\">\n\n            <a href=\"/w/wr/wr10_ACPS02-0603.djhtm\">\n\n             配息紀錄\n\n            </a>\n\n           </li>\n\n          </ul>\n\n         </div>\n\n         <script language=\"javascript\" src=\"/w/js/WtFundlistJS.djjs\">\n\n         </script>\n\n         <tr id=\"oScrollHead\">\n\n          <td class=\"wfb1c\" colspan=\"2\">\n\n           <script language=\"javascript\">\n\n            <!--\n\n var sObj = eval('document.' + 'wr02_frm'); \n\n\tiID = 'ACPS02-0603';\n\n\tGenFundCorpCombo('BFZPSA','ACPS02-0603','wr02_frm');\n\n\tfor (i=0;i<sObj.selTFund_corp.options.length;i++)\n\n\t{\n\n\t\tvar tmpID1 = sObj.selTFund_corp.options[i].value.toUpperCase();\n\n\t\tif (tmpID1 == 'BFZPSA') \n\n\t\t{\n\n\t\t\tsObj.selTFund_corp.selectedIndex = i;\n\n\t\t\tbreak;\n\n\t\t}\n\n\t}\n\n\tfor (i=0;i<sObj.selTFund3.options.length;i++)\n\n\t{\n\n\t\tvar tmpID2 = sObj.selTFund3.options[i].value.toUpperCase();\n\n\t\tif (iID != '')\n\n\t\t{\n\n\t\t\tif (tmpID2 == iID )\n\n\t\t\t{\n\n\t\t\t\tsObj.selTFund3.selectedIndex = i;\n\n\t\t\t\tbreak;\n\n\t\t\t}\n\n\t\t}\n\n\t\telse\n\n\t\t\tsObj.selTFund3.selectedIndex = 0;\n\n\t}\n\n//-->\n\n           </script>\n\n           <!--FundBuyBtn-->\n\n          </td>\n\n         </tr>\n\n         <tr>\n\n          <td class=\"wfb3c\" colspan=\"2\">\n\n           <table border=\"0\" cellpadding=\"1\" cellspacing=\"1\" class=\"wfb3l\" width=\"100%\">\n\n            <td class=\"wfb3c\" rowspan=\"2\">\n\n             請選擇期間\n\n            </td>\n\n            <td class=\"wfb3l\">\n\n             <input checked=\"\" name=\"radioMonth\" onclick=\"javascript:radioSelYear_onclick(true);\" type=\"radio\"/>\n\n             <select name=\"selYEAR\" onchange=\"javascript:SelYear_onChange(true,this.selectedIndex);\">\n\n              <option>\n\n               ＸＸＸＸＸ\n\n              </option>\n\n             </select>\n\n            </td>\n\n            <td class=\"wfb3c\" rowspan=\"2\">\n\n             <input name=\"BB1\" onclick=\"javascript:jumpgo();\" type=\"button\" value=\"開始查詢\"/>\n\n            </td>\n\n            <tr>\n\n             <td class=\"wfb3l\">\n\n              <input name=\"radioMonth\" onclick=\"javascript:radioSelYear_onclick(false);\" type=\"radio\"/>\n\n              <script language=\"JavaScript\" src=\"/w/js/month.js\">\n\n              </script>\n\n              從\n\n              <select name=\"Y2\" onchange=\"javascript:SetMonthDate(document.wr02_frm.Y2,document.wr02_frm.M2,document.wr02_frm.D2);\">\n\n               <option value=\"91\">\n\n               </option>\n\n              </select>\n\n              年\n\n              <select name=\"M2\" onchange=\"javascript:SetMonthDate(document.wr02_frm.Y2,document.wr02_frm.M2,document.wr02_frm.D2);\">\n\n               <option value=\"-1\">\n\n               </option>\n\n              </select>\n\n              月\n\n              <select name=\"D2\">\n\n               <option value=\"-1\">\n\n               </option>\n\n              </select>\n\n              日至\n\n              <select name=\"Y1\" onchange=\"javascript:SetMonthDate(document.wr02_frm.Y1,document.wr02_frm.M1,document.wr02_frm.D1);\">\n\n               <option value=\"91\">\n\n               </option>\n\n              </select>\n\n              年\n\n              <select name=\"M1\" onchange=\"javascript:SetMonthDate(document.wr02_frm.Y1,document.wr02_frm.M1,document.wr02_frm.D1);\">\n\n               <option value=\"-1\">\n\n               </option>\n\n              </select>\n\n              月\n\n              <select name=\"D1\">\n\n               <option value=\"-1\">\n\n               </option>\n\n              </select>\n\n              日\n\n              <script language=\"javascript\">\n\n               <!--\n\n\tvar getYMD1 = '2024-7-9';\n\n\tvar getYMD2 = '2023-7-9';\n\n   PageInit(document.wr02_frm);\n\n   function PageInit(obj){\t\t//在 BODY onLoad 時候的初始化\n\n\t\tShowYear(obj.Y1);\t\t\t\t\t\t\n\n\t\tShowYear(obj.Y2);\t\t\t\t\t\t\n\n\t\tSetOptionValue(obj.M1,1,12);\t\t\n\n\t\tSetOptionValue(obj.M2,1,12);\t\t\n\n\t\t//設定初始值\t\t\t\t\t\t\n\n\t\tvar YMDary1 = getYMD1.split('-');\t\n\n\t\tSetFocus(obj.Y1,YMDary1[0]);   \n\n\t\tSetFocus(obj.M1,YMDary1[1]);\t\t\n\n\t\tvar YMDary2 = getYMD2.split('-');\t\n\n\t\tSetFocus(obj.Y2,YMDary2[0]);   \n\n\t\tSetFocus(obj.M2,YMDary2[1]);\t\t\n\n\t\tSetMonthDate(obj.Y1,obj.M1,obj.D1);\t\t\n\n\t\tSetMonthDate(obj.Y2,obj.M2,obj.D2);\t\t\n\n\t\tSetFocus(obj.D2,YMDary2[2]);\t\t\n\n\t\tSetFocus(obj.D1,YMDary1[2]);\t\t\n\n\t}\t\t\t\t\t\t\t\t\t\t\n\n  function CheckSubmit (){\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\tvar Frm = document.wr02_frm;\t\t\t\t\t\t\t\t\t\t\n\n       var y1 = parseInt(Frm.Y1.options[Frm.Y1.selectedIndex].value);\t\n\n      \tvar m1 = parseInt(Frm.M1.options[Frm.M1.selectedIndex].value);\t\n\n      \tvar d1 = parseInt(Frm.D1.options[Frm.D1.selectedIndex].value);\t\n\n       var y2 = parseInt(Frm.Y2.options[Frm.Y2.selectedIndex].value);\t\n\n      \tvar m2 = parseInt(Frm.M2.options[Frm.M2.selectedIndex].value);\t\n\n      \tvar d2 = parseInt(Frm.D2.options[Frm.D2.selectedIndex].value);\t\n\n      \tvar sEDate = y1+'-'+m1+'-' + d1;\t\t\t\t\t\t\t\n\n      \tvar sBDate = y2+'-'+m2+'-' + d2;\t\t\t\t\t\t\t\n\n      \tif(checkBEdate(sBDate,sEDate)){\t\t\t\t\t\t\t\t\n\n\t\t\treturn sBDate + '_' + sEDate;\t\t\t\t\t\t\t\n\n\t\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\telse\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\treturn 'false';\t\t\t\t\t\t\t\t\t\t\t\n\n\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n  function checkBEdate(sBDate,sEDate){\t\t\t\t\t\t\t\t\t\t\t\t\n\n\tvar aymd1 = sBDate.split('-');\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\tvar aymd2 = sEDate.split('-');\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n   if (aymd1.length < 3 || aymd2.length < 3 ) {\t\t\t\t\t\t\t\t\t\n\n\t\t\talert ('日期選擇錯誤!!');\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\treturn false;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n   var nbdate = parseInt(aymd1[0])*10000+parseInt(aymd1[1])*100+parseInt(aymd1[2]);\n\n   var nedate = parseInt(aymd2[0])*10000+parseInt(aymd2[1])*100+parseInt(aymd2[2]);\n\n   if (nbdate > nedate) {\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\talert('您所輸入的起始日期大於結束日期');\t\t\t\t\t\t\t\t\t\n\n\t\t\treturn false;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t}\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t    return true;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n   }\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n//-->\n\n              </script>\n\n             </td>\n\n            </tr>\n\n           </table>\n\n          </td>\n\n         </tr>\n\n         <tr>\n\n          <td class=\"wfb5c\" colspan=\"2\">\n\n           <div id=\"SysJustWebGraphDIV\">\n\n           </div>\n\n          </td>\n\n         </tr>\n\n         <tr>\n\n          <td class=\"wfb1c\" colspan=\"2\">\n\n           統一黑馬基金-近30日淨值\n\n          </td>\n\n         </tr>\n\n         <tr>\n\n          <td class=\"wfb2c\" width=\"50%\">\n\n           <table cellpadding=\"1\" cellspacing=\"1\" class=\"wfb0c\" width=\"100%\">\n\n            <tr>\n\n             <td class=\"wfb3c\">\n\n              日期\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              淨值\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              漲/跌\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              漲跌幅(%)\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/07/09\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              236.31\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.76\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.75\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/07/08\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              234.55\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.56\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.24\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/07/05\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              235.11\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              3.01\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.3\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/07/04\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              232.10\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              4.32\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              1.9\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/07/03\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              227.78\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              2.17\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.96\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/07/02\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              225.61\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.85\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.38\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/07/01\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              226.46\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -2.45\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -1.07\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/28\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              228.91\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              3.19\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              1.41\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/27\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              225.72\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.96\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.42\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/26\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              226.68\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              4.15\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              1.86\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/25\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              222.53\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.17\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.08\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/24\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              222.36\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -5.42\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -2.38\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/21\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              227.78\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -2.67\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -1.16\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/20\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              230.45\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              3.81\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              1.68\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/19\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              226.64\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.53\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.68\n\n             </td>\n\n            </tr>\n\n           </table>\n\n          </td>\n\n          <td class=\"wfb2c\" width=\"50%\">\n\n           <table cellpadding=\"1\" cellspacing=\"1\" class=\"wfb0c\" width=\"100%\">\n\n            <tr>\n\n             <td class=\"wfb3c\">\n\n              日期\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              淨值\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              漲/跌\n\n             </td>\n\n             <td class=\"wfb3c\">\n\n              漲跌幅(%)\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/18\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              225.11\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.51\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.23\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/17\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              225.62\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.83\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.37\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/14\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              226.45\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              2.86\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.28\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/13\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              223.59\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              2.26\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              1.02\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/12\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              221.33\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              3.53\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.62\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/11\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              217.80\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.27\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.12\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/07\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              218.07\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.84\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.38\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/06\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              218.91\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              0.72\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              0.33\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/05\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              218.19\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.16\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -0.07\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/06/04\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              218.35\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -1.78\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -0.81\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/06/03\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              220.13\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              2.83\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              1.3\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/05/31\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              217.30\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -3.55\n\n             </td>\n\n             <td class=\"wfb2rr\">\n\n              -1.61\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/05/30\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              220.85\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -4.11\n\n             </td>\n\n             <td class=\"wfb5rr\">\n\n              -1.83\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb2c\">\n\n              2024/05/29\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              224.96\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              0.28\n\n             </td>\n\n             <td class=\"wfb2r\">\n\n              0.12\n\n             </td>\n\n            </tr>\n\n            <tr>\n\n             <td class=\"wfb5c\">\n\n              2024/05/28\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              224.68\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.52\n\n             </td>\n\n             <td class=\"wfb5r\">\n\n              0.23\n\n             </td>\n\n            </tr>\n\n           </table>\n\n          </td>\n\n         </tr>\n\n         <input name=\"as_sfid\" type=\"hidden\" value=\"AAAAAAU-aEY_7LLSztfxzQutuTo_liUeTKi1ShHKKT1uiy8NlROFAUn6ftfbchCwHJJ06MwntMAnJc9_baquu6-DKLcJbXeauHbhVGor_bZqQs4ujZFREeqE-EqqHjLFiT65divnnlojcQ9MycMRGEUJl_vUzeqg247uihqfP0FlobdvMg==\"/>\n\n         <input name=\"as_fid\" type=\"hidden\" value=\"6e302690f77c6153619ea65a7f1ac342ebb550a4\"/>\n\n        </form>\n\n       </table>\n\n       <script language=\"JavaScript\">\n\n        <!--\n\nvar sfund_year ='1@1@一年淨值@1@2@1@二年淨值@1@3@1@三年淨值@1@7@1@五年淨值@1@4@1@一個月淨值@1@5@1@三個月淨值@1@6@1@六個月淨值@1@';\n\nInitComboList(document.wr02_frm.selYEAR, '', '', '1', sfund_year, '請選擇');\n\nfunction radioSelYear_onclick(a) {\t\t\n\n     if (a) {\t\t\t\t\t\t\t\n\n\t\t\tdocument.wr02_frm.elements['Y1'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['Y2'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['M1'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['M2'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['D1'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['D2'].disabled = true;    \n\n\t\t\tdocument.wr02_frm.elements['selYEAR'].disabled = false;    \n\n     }else{\t\t\t\t\t\t\t\n\n\t\t\tdocument.wr02_frm.elements['Y1'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['Y2'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['M1'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['M2'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['D1'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['D2'].disabled = false;    \n\n\t\t\tdocument.wr02_frm.elements['selYEAR'].disabled = true;    \n\n\t\t}\t\t\t\t\t\t\t\t\n\n}\t\t\t\n\nfunction SelYear_onChange(a,b) {\t\t\n\n     if (a) {\t\t\t\t\t\t\t\n\n\t\t\tif (b > 0) {\t\t\t\t\t\n\n\t\t\t\tdocument.wr02_frm.elements['Y1'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['Y2'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['M1'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['M2'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['D1'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['D2'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['selYEAR'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['radioMonth'][0].checked = true;    \n\n\t\t\t}else{\t\t\t\t\t\t\t\n\n\t\t\t\tdocument.wr02_frm.elements['Y1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['Y2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['M1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['M2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['D1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['D2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['selYEAR'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['radioMonth'][0].checked = false;    \n\n\t\t\t}\t\t\t\t\t\t\t\t\n\n     }else{\t\t\t\t\t\t\t\n\n\t\t\t\tdocument.wr02_frm.elements['Y1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['Y2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['M1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['M2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['D1'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['D2'].disabled = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['selYEAR'].disabled = true;    \n\n\t\t\t\tdocument.wr02_frm.elements['radioMonth'][0].checked = false;    \n\n\t\t\t\tdocument.wr02_frm.elements['radioMonth'][1].checked = true;    \n\n\t\t}\t\t\t\t\t\t\t\t\n\n}\t\t\t\t\t\t\t\t\t\n\nfunction jumpgo(){ \n\n\tvar bcdurl = ''; \n\n\tvar mLink='';\n\n\tif (document.wr02_frm.elements['radioMonth'][0].checked && document.wr02_frm.elements['selYEAR'].selectedIndex > 0) {\n\n\t\tmLink = '_' + document.wr02_frm.elements['selYEAR'].options[document.wr02_frm.elements['selYEAR'].selectedIndex].value + '_0_0';\n\n\t}else if (document.wr02_frm.elements['radioMonth'][1].checked) {\n\n\t\tvar sTempDate= CheckSubmit() ;\n\n\t\tif (sTempDate != 'false')\t\n\n\t\t\tmLink = '_0_' + sTempDate;\t\n\n\t\telse return;\n\n\t}\n\n\tif (mLink == '') {\n\n\t\talert('請選擇要比較的區間');\n\n\t\tdocument.wr02_frm.elements['BB1'].disabled =  false;\n\n\t\treturn;\n\n\t}\n\n\tbcdurl = '/w/wr/wr02_ACPS02-0603' + mLink + '.djhtm';\n\n\tself.location = bcdurl ; \n\n} \n\n// -->\n\n       </script>\n\n      </td>\n\n     </tr>\n\n     <tr>\n\n      <td>\n\n      </td>\n\n     </tr>\n\n    </table>\n\n   </div>\n\n   <script language=\"javascript\" src=\"/w/js/SD.js\">\n\n   </script>\n\n  </center>\n\n  <script language=\"javascript\" src=\"/w/js/CelebrusInsertJS.djjs\">\n\n  </script>\n\n </body>\n\n</html>\n\n\n"},{"name":"stderr","output_type":"stream","text":"\n"}]},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = 'uniblack_stocks/3661世芯-KY.csv'\n\n# 讀取CSV檔案\ndata = pd.read_csv(file_path)\n\n# 新的列名\nnew_columns = ['index', 'date', 'volume', 'amount', 'open', 'high', 'low', 'close', 'change', 'transactions']\n\n# 修改資料框的列名\ndata.columns = new_columns\n\nprint(\"原始資料:\")\nprint(data)\n","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"原始資料:\n\n     index        date     volume         amount      open      high  \\\n\n0        1  2023/01/04  6,156,121  5,333,596,225    850.00    882.00   \n\n1        2  2023/01/05  6,189,016  5,410,239,243    888.00    897.00   \n\n2        3  2023/01/06  4,465,787  3,800,656,142    860.00    868.00   \n\n3        4  2023/01/09  5,261,930  4,521,987,667    859.00    873.00   \n\n4        5  2023/01/10  3,226,082  2,751,787,931    859.00    868.00   \n\n..     ...         ...        ...            ...       ...       ...   \n\n355      0  2024/07/01  1,843,413  4,573,201,995  2,470.00  2,510.00   \n\n356      1  2024/07/02  1,641,346  4,051,285,435  2,470.00  2,510.00   \n\n357      2  2024/07/03  2,216,019  5,476,201,385  2,485.00  2,535.00   \n\n358      3  2024/07/04  3,030,115  7,728,894,730  2,495.00  2,620.00   \n\n359      4  2024/07/05  2,928,024  7,463,372,970  2,615.00  2,625.00   \n\n\n\n          low     close   change transactions  \n\n0      844.00    879.00   +24.00        6,328  \n\n1      851.00    857.00   -22.00        9,301  \n\n2      831.00    837.00   -20.00        5,072  \n\n3      849.00    854.00   +17.00        5,159  \n\n4      836.00    851.00    -3.00        3,558  \n\n..        ...       ...      ...          ...  \n\n355  2,425.00  2,495.00   +40.00        6,157  \n\n356  2,445.00  2,455.00   -40.00        7,406  \n\n357  2,440.00  2,470.00   +15.00        7,339  \n\n358  2,485.00  2,615.00  +145.00       10,481  \n\n359  2,510.00  2,525.00   -90.00       14,219  \n\n\n\n[360 rows x 10 columns]\n"}]},{"cell_type":"code","source":"pip install scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-07-16T11:31:16.828336Z","iopub.execute_input":"2024-07-16T11:31:16.828839Z","iopub.status.idle":"2024-07-16T11:31:33.129420Z","shell.execute_reply.started":"2024-07-16T11:31:16.828797Z","shell.execute_reply":"2024-07-16T11:31:33.127642Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\n\n# 假設 data 已經被讀取進來\n# 排除不需要的欄位\ncolumns_to_exclude = ['index', 'date', 'change']\ndata_to_normalize = data.drop(columns=columns_to_exclude)\n\n# 確保要標準化的欄位是數值型別，移除千分位逗號並轉換為 float\nfor column in ['volume', 'amount', 'transactions']:\n    data_to_normalize[column] = data_to_normalize[column].str.replace(',', '').astype(float)\n\n# 轉換剩餘的欄位為 float 型別\nfor column in ['open', 'high', 'low', 'close']:\n    data_to_normalize[column] = data_to_normalize[column].str.replace(',', '').astype(float)\n\n# 標準化處理\nscaler = StandardScaler()\nnormalized_data = scaler.fit_transform(data_to_normalize)\n\n# 轉換為 DataFrame\nclean_data = pd.DataFrame(normalized_data, columns=data_to_normalize.columns)\n\n# 顯示結果\nprint(clean_data)\n","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"       volume    amount      open      high       low     close  transactions\n\n0    1.635717 -0.500859 -1.652797 -1.649647 -1.642413 -1.630389      0.016670\n\n1    1.654212 -0.475771 -1.612918 -1.634112 -1.634848 -1.653806      0.789501\n\n2    0.685348 -1.002634 -1.642302 -1.664145 -1.656460 -1.675094     -0.309827\n\n3    1.132969 -0.766521 -1.643352 -1.658967 -1.637010 -1.656999     -0.287211\n\n4   -0.011661 -1.345958 -1.643352 -1.664145 -1.651057 -1.660192     -0.703391\n\n..        ...       ...       ...       ...       ...       ...           ...\n\n355 -0.789050 -0.749757  0.047316  0.036353  0.065994  0.089679     -0.027781\n\n356 -0.902659 -0.920596  0.047316  0.036353  0.087606  0.047103      0.296896\n\n357 -0.579557 -0.454180  0.063057  0.062244  0.082203  0.063069      0.279479\n\n358 -0.121841  0.283191  0.073552  0.150272  0.130829  0.217406      1.096242\n\n359 -0.179241  0.196278  0.199486  0.155450  0.157844  0.121610      2.067934\n\n\n\n[360 rows x 7 columns]\n"}]},{"cell_type":"code","source":"aa=pd.read_html('uni_hor.html')\n\ndf_a=aa[2]\ndf_b=aa[3]\n\n# 移除表格 A 和 B 的第一行（標題行）進行上下合併\ndf_a_content = df_a.iloc[1:].reset_index(drop=True)\ndf_b_content = df_b.iloc[1:].reset_index(drop=True)\n\n# 合併表格\ncombined_df = pd.concat([df_a_content, df_b_content], ignore_index=True)\n\n# 設置標題行\ncombined_df.columns = df_a.iloc[0]\n\n# 顯示合併後的 DataFrame\nprint(combined_df)","metadata":{},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"0           日期      淨值    漲/跌 漲跌幅(%)\n\n0   2024/07/09  236.31   1.76   0.75\n\n1   2024/07/08  234.55  -0.56  -0.24\n\n2   2024/07/05  235.11   3.01    1.3\n\n3   2024/07/04  232.10   4.32    1.9\n\n4   2024/07/03  227.78   2.17   0.96\n\n5   2024/07/02  225.61  -0.85  -0.38\n\n6   2024/07/01  226.46  -2.45  -1.07\n\n7   2024/06/28  228.91   3.19   1.41\n\n8   2024/06/27  225.72  -0.96  -0.42\n\n9   2024/06/26  226.68   4.15   1.86\n\n10  2024/06/25  222.53   0.17   0.08\n\n11  2024/06/24  222.36  -5.42  -2.38\n\n12  2024/06/21  227.78  -2.67  -1.16\n\n13  2024/06/20  230.45   3.81   1.68\n\n14  2024/06/19  226.64   1.53   0.68\n\n15  2024/06/18  225.11  -0.51  -0.23\n\n16  2024/06/17  225.62  -0.83  -0.37\n\n17  2024/06/14  226.45   2.86   1.28\n\n18  2024/06/13  223.59   2.26   1.02\n\n19  2024/06/12  221.33   3.53   1.62\n\n20  2024/06/11  217.80  -0.27  -0.12\n\n21  2024/06/07  218.07  -0.84  -0.38\n\n22  2024/06/06  218.91   0.72   0.33\n\n23  2024/06/05  218.19  -0.16  -0.07\n\n24  2024/06/04  218.35  -1.78  -0.81\n\n25  2024/06/03  220.13   2.83    1.3\n\n26  2024/05/31  217.30  -3.55  -1.61\n\n27  2024/05/30  220.85  -4.11  -1.83\n\n28  2024/05/29  224.96   0.28   0.12\n\n29  2024/05/28  224.68   0.52   0.23\n"}]},{"cell_type":"code","source":"needed_data=clean_data[-30:-1]\n\n# 生成滑動窗口的資料組\ntrain_data_x = [needed_data.iloc[i:i+10] for i in range(len(needed_data) - 9)]\n\n'''\n# 列印每組資料\nfor i, group in enumerate(train_data_x):\n    print(f\"Group {i+1}:\\n{group}\\n\")\n'''\n\nprint(train_data_x[19])\nprint(len(train_data_x))","metadata":{},"execution_count":30,"outputs":[{"name":"stdout","output_type":"stream","text":"       volume    amount      open      high       low     close  transactions\n\n349  0.192450  0.936533  0.377893  0.362576  0.319931  0.286592      2.453180\n\n350 -0.741655 -0.572503  0.251959  0.238301  0.255096  0.201440      1.987610\n\n351 -0.466068 -0.216241  0.183744  0.139916  0.179455  0.174830      1.921583\n\n352 -0.078908  0.339435  0.204734  0.155450  0.163247  0.121610      2.921090\n\n353  0.889733  1.622960  0.068305  0.036353  0.049785  0.041781      5.232824\n\n354 -0.684105 -0.613598  0.026327  0.020819  0.082203  0.047103      0.398536\n\n355 -0.789050 -0.749757  0.047316  0.036353  0.065994  0.089679     -0.027781\n\n356 -0.902659 -0.920596  0.047316  0.036353  0.087606  0.047103      0.296896\n\n357 -0.579557 -0.454180  0.063057  0.062244  0.082203  0.063069      0.279479\n\n358 -0.121841  0.283191  0.073552  0.150272  0.130829  0.217406      1.096242\n\n20\n"}]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# 使用 MinMaxScaler 進行標準化\nscaler = MinMaxScaler()\ncombined_df['淨值'] = scaler.fit_transform(combined_df[['淨值']])\n\n# 將標準化後的 '淨值' 欄位乘以 100 並轉換為整數\ncombined_df['淨值'] = (combined_df['淨值'] * 100).astype(int)\n\n# 只保留 '淨值' 欄位，並且去除 '日期', '漲/跌', '漲跌幅(%)' 欄位\ndata_y = combined_df[['淨值']]\n\nprint(data_y)","metadata":{},"execution_count":31,"outputs":[{"name":"stdout","output_type":"stream","text":"0    淨值\n\n0   100\n\n1    90\n\n2    93\n\n3    77\n\n4    55\n\n5    43\n\n6    48\n\n7    61\n\n8    44\n\n9    49\n\n10   27\n\n11   26\n\n12   55\n\n13   69\n\n14   49\n\n15   41\n\n16   43\n\n17   48\n\n18   33\n\n19   21\n\n20    2\n\n21    4\n\n22    8\n\n23    4\n\n24    5\n\n25   14\n\n26    0\n\n27   18\n\n28   40\n\n29   38\n"}]},{"cell_type":"code","source":"# 提取所需的值\ndata_y_now=[]\ndata_y_future=[]\n\nfor i in range(29,9,-1):\n    data_y_now.append(data_y['淨值'].iloc[i])\n    data_y_future.append(data_y['淨值'].iloc[i-10])\n\nprint(f\"data_y_now: {data_y_now}\")\nprint(f\"data_y_future: {data_y_future}\")","metadata":{},"execution_count":32,"outputs":[{"name":"stdout","output_type":"stream","text":"data_y_now: [38, 40, 18, 0, 14, 5, 4, 8, 4, 2, 21, 33, 48, 43, 41, 49, 69, 55, 26, 27]\n\ndata_y_future: [21, 33, 48, 43, 41, 49, 69, 55, 26, 27, 49, 44, 61, 48, 43, 55, 77, 93, 90, 100]\n"}]},{"cell_type":"code","source":"len(data_y_future)","metadata":{},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ny_label=[]\n\nfor i in range(len(data_y_future)):\n    if data_y_now[i]>=data_y_future[i]:\n        y_label.append(0)   #不要進場\n    else:\n        y_label.append(1)   #進場\n\nprint(train_data_x[0].shape)\n","metadata":{},"execution_count":34,"outputs":[{"name":"stdout","output_type":"stream","text":"(10, 7)\n"}]},{"cell_type":"code","source":"y_label","metadata":{},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":["[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\n\nx_train, x_temp, y_train, y_temp = tts(train_data_x, y_label, test_size = 0.2,random_state=7100) #test_size可改\nx_valid, x_test, y_valid, y_test = tts(x_temp, y_temp, test_size = 0.5,random_state=7100)","metadata":{},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"pip install torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n#import seaborn as sns\n\n# 定義 TCN 模型\nclass TCN(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TCN, self).__init__()\n        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        self.fc1 = nn.Linear(192, 128)\n        self.fc2 = nn.Linear(128, output_size)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# 示例輸入形狀 (batch_size, input_size, sequence_length)\ninput_size = 10\noutput_size = 2\n\nmodel = TCN(input_size, output_size)\n\n# 定義損失函數和優化器\ncriterion = nn.BCEWithLogitsLoss()  # Binary cross entropy with logits for one-hot encoded targets\noptimizer = optim.Adam(model.parameters(), lr=0.0003)\n\n","metadata":{},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# 假设类别数量为2\nnum_classes = 2\n\n\n# 将标签转换为 Tensor 类型\ny_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\n\n# 将标签转换为 one-hot 编码\ny_valid_one_hot = F.one_hot(y_valid_tensor, num_classes)\ny_train_one_hot = F.one_hot(y_train_tensor, num_classes)\n\nprint(y_valid_one_hot)\n","metadata":{},"execution_count":38,"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0, 1],\n\n        [0, 1]])\n"}]},{"cell_type":"code","source":"# 去除 index 和欄位名稱\n\n# 將每個 DataFrame 轉換為 numpy array 然後轉換為 tensor\nx_train_tensors = [torch.tensor(df.values, dtype=torch.float32) for df in x_train]\nx_valid_tensors = [torch.tensor(df.values, dtype=torch.float32) for df in x_valid]\nx_test_tensors = [torch.tensor(df.values, dtype=torch.float32) for df in x_test]\n\n# 合併所有 tensor 成為一個大的 tensor（如果需要）\nx_train_tensor = torch.stack(x_train_tensors)\nx_valid_tensor = torch.stack(x_valid_tensors)\nx_test_tensor = torch.stack(x_test_tensors)\n\n#y_train_tensor = torch.LongTensor(y_train)\n#y_valid_tensor = torch.LongTensor(y_valid)\n\n# 創建 DataLoader\ntrain_dataset = TensorDataset(x_train_tensor, y_train_one_hot)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\n\n# 創建 DataLoader\nvalid_dataset = TensorDataset(x_valid_tensor, y_valid_one_hot)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=True)\n","metadata":{},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n#import matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n# 訓練模型\n# 訓練設置\nnum_epochs = 1000\nloss_values = []\nval_loss_values = []\nbest_val_loss = float('inf')\nbest_accuracy = 0.0\npatience = 50\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    total_train_loss = 0.0\n    total_val_loss = 0.0\n\n    # 訓練模式\n    model.train()\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        labels = labels.float()  # Ensure labels match outputs shape\n        #print(outputs,labels)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_train_loss += loss.item()  # 累加每個 batch 的損失值\n\n    epoch_train_loss = total_train_loss / len(train_loader)  # 計算每個 epoch 的平均訓練損失值\n    loss_values.append(epoch_train_loss)  # 將平均訓練損失值添加到列表中\n\n    # 驗證模式\n    model.eval()\n    val_predictions = []\n    val_labels = []\n    with torch.no_grad():\n        for inputs, labels in valid_loader:\n            outputs = model(inputs)\n            labels = labels.float() # Ensure labels match outputs shape\n            loss = criterion(outputs, labels)\n            total_val_loss += loss.item()  # 累加每個 batch 的損失值\n            val_predictions.append(outputs)\n            val_labels.append(labels)\n    epoch_val_loss = total_val_loss / len(valid_loader)  # 計算每個 epoch 的平均驗證損失值\n    val_loss_values.append(epoch_val_loss)  # 將平均驗證損失值添加到列表中\n\n    # 計算 accuracy（根據需要調整）\n    #accuracy = mean_squared_error(val_labels, val_predictions)\n    # Convert lists to tensors\n    val_predictions = torch.cat(val_predictions)\n    val_labels = torch.cat(val_labels)\n    \n    # Convert logits to probabilities and round to get binary predictions\n    val_predictions_rounded = torch.round(torch.sigmoid(val_predictions))\n\n    # Calculate accuracy\n    accuracy = accuracy_score(val_labels.cpu().numpy(), val_predictions_rounded.cpu().numpy())\n\n\n    # 儲存最佳模型\n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        best_accuracy = accuracy\n        torch.save(model.state_dict(), 'best_model.pth')\n        print('model saved.') \n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.8f}, Validation Loss: {epoch_val_loss:.8f}, Accuracy: {accuracy:.4f}')\n\n    # 提早停止\n    if early_stop_counter >= patience:\n        print(\"Early stopping\")\n        break\n","metadata":{"tags":[],"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":40,"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch 1/1000, Train Loss: 0.64777848, Validation Loss: 0.59302706, Accuracy: 1.0000\n\nEpoch 2/1000, Train Loss: 0.58046660, Validation Loss: 0.48402493, Accuracy: 1.0000\n\nEpoch 3/1000, Train Loss: 0.50401909, Validation Loss: 0.38571577, Accuracy: 1.0000\n\nEpoch 4/1000, Train Loss: 0.43058169, Validation Loss: 0.31112990, Accuracy: 1.0000\n\nEpoch 5/1000, Train Loss: 0.38277983, Validation Loss: 0.21986410, Accuracy: 1.0000\n\nEpoch 6/1000, Train Loss: 0.34103275, Validation Loss: 0.18123560, Accuracy: 1.0000\n\nEpoch 7/1000, Train Loss: 0.31666004, Validation Loss: 0.14509260, Accuracy: 1.0000\n\nEpoch 8/1000, Train Loss: 0.30308020, Validation Loss: 0.12235502, Accuracy: 1.0000\n\nEpoch 9/1000, Train Loss: 0.28640549, Validation Loss: 0.11253177, Accuracy: 1.0000\n\nEpoch 10/1000, Train Loss: 0.26812540, Validation Loss: 0.10559167, Accuracy: 1.0000\n\nEpoch 11/1000, Train Loss: 0.25215470, Validation Loss: 0.10012814, Accuracy: 1.0000\n\nEpoch 12/1000, Train Loss: 0.23927913, Validation Loss: 0.09618430, Accuracy: 1.0000\n\nEpoch 13/1000, Train Loss: 0.22250697, Validation Loss: 0.09125514, Accuracy: 1.0000\n\nEpoch 14/1000, Train Loss: 0.20636501, Validation Loss: 0.07572251, Accuracy: 1.0000\n\nEpoch 15/1000, Train Loss: 0.18471845, Validation Loss: 0.07164732, Accuracy: 1.0000\n\nEpoch 16/1000, Train Loss: 0.17210883, Validation Loss: 0.07094688, Accuracy: 1.0000\n\nEpoch 17/1000, Train Loss: 0.15014674, Validation Loss: 0.05844094, Accuracy: 1.0000\n\nEpoch 18/1000, Train Loss: 0.13495005, Validation Loss: 0.04681173, Accuracy: 1.0000\n\nEpoch 19/1000, Train Loss: 0.11722210, Validation Loss: 0.04215210, Accuracy: 1.0000\n\nEpoch 20/1000, Train Loss: 0.10330768, Validation Loss: 0.03759739, Accuracy: 1.0000\n\nEpoch 21/1000, Train Loss: 0.08996123, Validation Loss: 0.03197748, Accuracy: 1.0000\n\nEpoch 22/1000, Train Loss: 0.07807576, Validation Loss: 0.02618923, Accuracy: 1.0000\n\nEpoch 23/1000, Train Loss: 0.06900719, Validation Loss: 0.02322769, Accuracy: 1.0000\n\nEpoch 24/1000, Train Loss: 0.05897419, Validation Loss: 0.01781735, Accuracy: 1.0000\n\nEpoch 25/1000, Train Loss: 0.05149786, Validation Loss: 0.01564897, Accuracy: 1.0000\n\nEpoch 26/1000, Train Loss: 0.04527889, Validation Loss: 0.01344566, Accuracy: 1.0000\n\nEpoch 27/1000, Train Loss: 0.04045395, Validation Loss: 0.01205290, Accuracy: 1.0000\n\nEpoch 28/1000, Train Loss: 0.03450982, Validation Loss: 0.00954205, Accuracy: 1.0000\n\nEpoch 29/1000, Train Loss: 0.03041609, Validation Loss: 0.00830533, Accuracy: 1.0000\n\nEpoch 30/1000, Train Loss: 0.02686861, Validation Loss: 0.00732847, Accuracy: 1.0000\n\nEpoch 31/1000, Train Loss: 0.02419749, Validation Loss: 0.00617973, Accuracy: 1.0000\n\nEpoch 32/1000, Train Loss: 0.02130411, Validation Loss: 0.00526995, Accuracy: 1.0000\n\nEpoch 33/1000, Train Loss: 0.01911504, Validation Loss: 0.00476875, Accuracy: 1.0000\n\nEpoch 34/1000, Train Loss: 0.01740138, Validation Loss: 0.00434287, Accuracy: 1.0000\n\nEpoch 35/1000, Train Loss: 0.01560600, Validation Loss: 0.00372117, Accuracy: 1.0000\n\nEpoch 36/1000, Train Loss: 0.01416747, Validation Loss: 0.00321631, Accuracy: 1.0000\n\nEpoch 37/1000, Train Loss: 0.01312922, Validation Loss: 0.00309910, Accuracy: 1.0000\n\nEpoch 38/1000, Train Loss: 0.01174041, Validation Loss: 0.00276440, Accuracy: 1.0000\n\nEpoch 39/1000, Train Loss: 0.01081696, Validation Loss: 0.00248446, Accuracy: 1.0000\n\nEpoch 40/1000, Train Loss: 0.00974055, Validation Loss: 0.00224574, Accuracy: 1.0000\n\nEpoch 41/1000, Train Loss: 0.00903624, Validation Loss: 0.00201600, Accuracy: 1.0000\n\nEpoch 42/1000, Train Loss: 0.00837729, Validation Loss: 0.00187553, Accuracy: 1.0000\n\nEpoch 43/1000, Train Loss: 0.00774304, Validation Loss: 0.00170182, Accuracy: 1.0000\n\nEpoch 44/1000, Train Loss: 0.00719548, Validation Loss: 0.00156894, Accuracy: 1.0000\n\nEpoch 45/1000, Train Loss: 0.00665065, Validation Loss: 0.00142218, Accuracy: 1.0000\n\nEpoch 46/1000, Train Loss: 0.00616158, Validation Loss: 0.00131575, Accuracy: 1.0000\n\nEpoch 47/1000, Train Loss: 0.00587595, Validation Loss: 0.00123145, Accuracy: 1.0000\n\nEpoch 48/1000, Train Loss: 0.00542119, Validation Loss: 0.00113058, Accuracy: 1.0000\n\nEpoch 49/1000, Train Loss: 0.00508832, Validation Loss: 0.00108200, Accuracy: 1.0000\n\nEpoch 50/1000, Train Loss: 0.00475136, Validation Loss: 0.00099194, Accuracy: 1.0000\n\nEpoch 51/1000, Train Loss: 0.00447472, Validation Loss: 0.00090624, Accuracy: 1.0000\n\nEpoch 52/1000, Train Loss: 0.00420544, Validation Loss: 0.00087505, Accuracy: 1.0000\n\nEpoch 53/1000, Train Loss: 0.00397553, Validation Loss: 0.00082053, Accuracy: 1.0000\n\nEpoch 54/1000, Train Loss: 0.00375854, Validation Loss: 0.00075909, Accuracy: 1.0000\n\nEpoch 55/1000, Train Loss: 0.00357296, Validation Loss: 0.00072647, Accuracy: 1.0000\n\nEpoch 56/1000, Train Loss: 0.00334685, Validation Loss: 0.00067438, Accuracy: 1.0000\n\nEpoch 57/1000, Train Loss: 0.00319467, Validation Loss: 0.00063871, Accuracy: 1.0000\n\nEpoch 58/1000, Train Loss: 0.00302742, Validation Loss: 0.00059888, Accuracy: 1.0000\n\nEpoch 59/1000, Train Loss: 0.00288787, Validation Loss: 0.00057482, Accuracy: 1.0000\n\nEpoch 60/1000, Train Loss: 0.00272657, Validation Loss: 0.00054189, Accuracy: 1.0000\n\nEpoch 61/1000, Train Loss: 0.00260796, Validation Loss: 0.00051163, Accuracy: 1.0000\n\nEpoch 62/1000, Train Loss: 0.00250533, Validation Loss: 0.00049296, Accuracy: 1.0000\n\nEpoch 63/1000, Train Loss: 0.00237085, Validation Loss: 0.00046065, Accuracy: 1.0000\n\nEpoch 64/1000, Train Loss: 0.00229110, Validation Loss: 0.00044279, Accuracy: 1.0000\n\nEpoch 65/1000, Train Loss: 0.00217034, Validation Loss: 0.00042387, Accuracy: 1.0000\n\nEpoch 66/1000, Train Loss: 0.00207099, Validation Loss: 0.00040285, Accuracy: 1.0000\n\nEpoch 67/1000, Train Loss: 0.00199571, Validation Loss: 0.00038018, Accuracy: 1.0000\n\nEpoch 68/1000, Train Loss: 0.00190508, Validation Loss: 0.00036538, Accuracy: 1.0000\n\nEpoch 69/1000, Train Loss: 0.00182664, Validation Loss: 0.00035365, Accuracy: 1.0000\n\nEpoch 70/1000, Train Loss: 0.00174846, Validation Loss: 0.00033515, Accuracy: 1.0000\n\nEpoch 71/1000, Train Loss: 0.00169164, Validation Loss: 0.00032044, Accuracy: 1.0000\n\nEpoch 72/1000, Train Loss: 0.00162065, Validation Loss: 0.00030498, Accuracy: 1.0000\n\nEpoch 73/1000, Train Loss: 0.00155556, Validation Loss: 0.00029497, Accuracy: 1.0000\n\nEpoch 74/1000, Train Loss: 0.00151123, Validation Loss: 0.00028270, Accuracy: 1.0000\n\nEpoch 75/1000, Train Loss: 0.00143866, Validation Loss: 0.00027164, Accuracy: 1.0000\n\nEpoch 76/1000, Train Loss: 0.00138769, Validation Loss: 0.00026274, Accuracy: 1.0000\n\nEpoch 77/1000, Train Loss: 0.00136309, Validation Loss: 0.00025431, Accuracy: 1.0000\n\nEpoch 78/1000, Train Loss: 0.00128848, Validation Loss: 0.00024191, Accuracy: 1.0000\n\nEpoch 79/1000, Train Loss: 0.00124935, Validation Loss: 0.00023491, Accuracy: 1.0000\n\nEpoch 80/1000, Train Loss: 0.00120977, Validation Loss: 0.00022758, Accuracy: 1.0000\n\nEpoch 81/1000, Train Loss: 0.00115878, Validation Loss: 0.00021683, Accuracy: 1.0000\n\nEpoch 82/1000, Train Loss: 0.00112225, Validation Loss: 0.00020894, Accuracy: 1.0000\n\nEpoch 83/1000, Train Loss: 0.00108776, Validation Loss: 0.00020080, Accuracy: 1.0000\n\nEpoch 84/1000, Train Loss: 0.00105104, Validation Loss: 0.00019508, Accuracy: 1.0000\n\nEpoch 85/1000, Train Loss: 0.00102323, Validation Loss: 0.00018975, Accuracy: 1.0000\n\nEpoch 86/1000, Train Loss: 0.00098134, Validation Loss: 0.00018096, Accuracy: 1.0000\n\nEpoch 87/1000, Train Loss: 0.00095181, Validation Loss: 0.00017536, Accuracy: 1.0000\n\nEpoch 88/1000, Train Loss: 0.00092435, Validation Loss: 0.00017041, Accuracy: 1.0000\n\nEpoch 89/1000, Train Loss: 0.00089436, Validation Loss: 0.00016350, Accuracy: 1.0000\n\nEpoch 90/1000, Train Loss: 0.00087130, Validation Loss: 0.00015948, Accuracy: 1.0000\n\nEpoch 91/1000, Train Loss: 0.00084133, Validation Loss: 0.00015474, Accuracy: 1.0000\n\nEpoch 92/1000, Train Loss: 0.00081725, Validation Loss: 0.00015054, Accuracy: 1.0000\n\nEpoch 93/1000, Train Loss: 0.00079523, Validation Loss: 0.00014470, Accuracy: 1.0000\n\nEpoch 94/1000, Train Loss: 0.00077290, Validation Loss: 0.00014068, Accuracy: 1.0000\n\nEpoch 95/1000, Train Loss: 0.00074691, Validation Loss: 0.00013547, Accuracy: 1.0000\n\nEpoch 96/1000, Train Loss: 0.00072543, Validation Loss: 0.00013198, Accuracy: 1.0000\n\nEpoch 97/1000, Train Loss: 0.00070991, Validation Loss: 0.00012825, Accuracy: 1.0000\n\nEpoch 98/1000, Train Loss: 0.00068360, Validation Loss: 0.00012483, Accuracy: 1.0000\n\nEpoch 99/1000, Train Loss: 0.00066669, Validation Loss: 0.00012110, Accuracy: 1.0000\n\nEpoch 100/1000, Train Loss: 0.00064907, Validation Loss: 0.00011723, Accuracy: 1.0000\n\nEpoch 101/1000, Train Loss: 0.00063373, Validation Loss: 0.00011416, Accuracy: 1.0000\n\nEpoch 102/1000, Train Loss: 0.00061421, Validation Loss: 0.00011062, Accuracy: 1.0000\n\nEpoch 103/1000, Train Loss: 0.00059798, Validation Loss: 0.00010781, Accuracy: 1.0000\n\nEpoch 104/1000, Train Loss: 0.00058299, Validation Loss: 0.00010513, Accuracy: 1.0000\n\nEpoch 105/1000, Train Loss: 0.00056704, Validation Loss: 0.00010156, Accuracy: 1.0000\n\nEpoch 106/1000, Train Loss: 0.00055353, Validation Loss: 0.00009956, Accuracy: 1.0000\n\nEpoch 107/1000, Train Loss: 0.00054002, Validation Loss: 0.00009631, Accuracy: 1.0000\n\nEpoch 108/1000, Train Loss: 0.00052491, Validation Loss: 0.00009444, Accuracy: 1.0000\n\nEpoch 109/1000, Train Loss: 0.00051163, Validation Loss: 0.00009143, Accuracy: 1.0000\n\nEpoch 110/1000, Train Loss: 0.00050266, Validation Loss: 0.00008919, Accuracy: 1.0000\n\nEpoch 111/1000, Train Loss: 0.00048788, Validation Loss: 0.00008740, Accuracy: 1.0000\n\nEpoch 112/1000, Train Loss: 0.00047576, Validation Loss: 0.00008496, Accuracy: 1.0000\n\nEpoch 113/1000, Train Loss: 0.00046290, Validation Loss: 0.00008252, Accuracy: 1.0000\n\nEpoch 114/1000, Train Loss: 0.00045318, Validation Loss: 0.00008100, Accuracy: 1.0000\n\nEpoch 115/1000, Train Loss: 0.00044518, Validation Loss: 0.00007805, Accuracy: 1.0000\n\nEpoch 116/1000, Train Loss: 0.00043208, Validation Loss: 0.00007745, Accuracy: 1.0000\n\nEpoch 117/1000, Train Loss: 0.00042218, Validation Loss: 0.00007459, Accuracy: 1.0000\n\nEpoch 118/1000, Train Loss: 0.00041221, Validation Loss: 0.00007247, Accuracy: 1.0000\n\nEpoch 119/1000, Train Loss: 0.00040313, Validation Loss: 0.00007191, Accuracy: 1.0000\n\nEpoch 120/1000, Train Loss: 0.00039418, Validation Loss: 0.00007006, Accuracy: 1.0000\n\nEpoch 121/1000, Train Loss: 0.00038474, Validation Loss: 0.00006780, Accuracy: 1.0000\n\nEpoch 122/1000, Train Loss: 0.00037495, Validation Loss: 0.00006595, Accuracy: 1.0000\n\nEpoch 123/1000, Train Loss: 0.00036686, Validation Loss: 0.00006473, Accuracy: 1.0000\n\nEpoch 124/1000, Train Loss: 0.00035894, Validation Loss: 0.00006353, Accuracy: 1.0000\n\nEpoch 125/1000, Train Loss: 0.00035159, Validation Loss: 0.00006181, Accuracy: 1.0000\n\nEpoch 126/1000, Train Loss: 0.00034341, Validation Loss: 0.00006035, Accuracy: 1.0000\n\nEpoch 127/1000, Train Loss: 0.00033679, Validation Loss: 0.00005933, Accuracy: 1.0000\n\nEpoch 128/1000, Train Loss: 0.00032897, Validation Loss: 0.00005781, Accuracy: 1.0000\n\nEpoch 129/1000, Train Loss: 0.00032254, Validation Loss: 0.00005641, Accuracy: 1.0000\n\nEpoch 130/1000, Train Loss: 0.00031480, Validation Loss: 0.00005555, Accuracy: 1.0000\n\nEpoch 131/1000, Train Loss: 0.00030941, Validation Loss: 0.00005424, Accuracy: 1.0000\n\nEpoch 132/1000, Train Loss: 0.00030173, Validation Loss: 0.00005284, Accuracy: 1.0000\n\nEpoch 133/1000, Train Loss: 0.00029586, Validation Loss: 0.00005167, Accuracy: 1.0000\n\nEpoch 134/1000, Train Loss: 0.00029045, Validation Loss: 0.00005087, Accuracy: 1.0000\n\nEpoch 135/1000, Train Loss: 0.00028396, Validation Loss: 0.00004950, Accuracy: 1.0000\n\nEpoch 136/1000, Train Loss: 0.00027849, Validation Loss: 0.00004858, Accuracy: 1.0000\n\nEpoch 137/1000, Train Loss: 0.00027218, Validation Loss: 0.00004729, Accuracy: 1.0000\n\nEpoch 138/1000, Train Loss: 0.00026736, Validation Loss: 0.00004634, Accuracy: 1.0000\n\nEpoch 139/1000, Train Loss: 0.00026138, Validation Loss: 0.00004557, Accuracy: 1.0000\n\nEpoch 140/1000, Train Loss: 0.00025654, Validation Loss: 0.00004467, Accuracy: 1.0000\n\nEpoch 141/1000, Train Loss: 0.00025233, Validation Loss: 0.00004369, Accuracy: 1.0000\n\nEpoch 142/1000, Train Loss: 0.00024646, Validation Loss: 0.00004273, Accuracy: 1.0000\n\nEpoch 143/1000, Train Loss: 0.00024162, Validation Loss: 0.00004187, Accuracy: 1.0000\n\nEpoch 144/1000, Train Loss: 0.00023717, Validation Loss: 0.00004101, Accuracy: 1.0000\n\nEpoch 145/1000, Train Loss: 0.00023400, Validation Loss: 0.00004005, Accuracy: 1.0000\n\nEpoch 146/1000, Train Loss: 0.00022755, Validation Loss: 0.00003937, Accuracy: 1.0000\n\nEpoch 147/1000, Train Loss: 0.00022365, Validation Loss: 0.00003859, Accuracy: 1.0000\n\nEpoch 148/1000, Train Loss: 0.00021913, Validation Loss: 0.00003791, Accuracy: 1.0000\n\nEpoch 149/1000, Train Loss: 0.00021529, Validation Loss: 0.00003698, Accuracy: 1.0000\n\nEpoch 150/1000, Train Loss: 0.00021178, Validation Loss: 0.00003642, Accuracy: 1.0000\n\nEpoch 151/1000, Train Loss: 0.00020730, Validation Loss: 0.00003570, Accuracy: 1.0000\n\nEpoch 152/1000, Train Loss: 0.00020397, Validation Loss: 0.00003499, Accuracy: 1.0000\n\nEpoch 153/1000, Train Loss: 0.00019950, Validation Loss: 0.00003427, Accuracy: 1.0000\n\nEpoch 154/1000, Train Loss: 0.00019749, Validation Loss: 0.00003329, Accuracy: 1.0000\n\nEpoch 155/1000, Train Loss: 0.00019248, Validation Loss: 0.00003305, Accuracy: 1.0000\n\nEpoch 156/1000, Train Loss: 0.00018981, Validation Loss: 0.00003248, Accuracy: 1.0000\n\nEpoch 157/1000, Train Loss: 0.00018547, Validation Loss: 0.00003183, Accuracy: 1.0000\n\nEpoch 158/1000, Train Loss: 0.00018266, Validation Loss: 0.00003123, Accuracy: 1.0000\n\nEpoch 159/1000, Train Loss: 0.00017889, Validation Loss: 0.00003052, Accuracy: 1.0000\n\nEpoch 160/1000, Train Loss: 0.00017595, Validation Loss: 0.00003010, Accuracy: 1.0000\n\nEpoch 161/1000, Train Loss: 0.00017257, Validation Loss: 0.00002938, Accuracy: 1.0000\n\nEpoch 162/1000, Train Loss: 0.00017020, Validation Loss: 0.00002903, Accuracy: 1.0000\n\nEpoch 163/1000, Train Loss: 0.00016723, Validation Loss: 0.00002828, Accuracy: 1.0000\n\nEpoch 164/1000, Train Loss: 0.00016408, Validation Loss: 0.00002786, Accuracy: 1.0000\n\nEpoch 165/1000, Train Loss: 0.00016129, Validation Loss: 0.00002730, Accuracy: 1.0000\n\nEpoch 166/1000, Train Loss: 0.00015855, Validation Loss: 0.00002685, Accuracy: 1.0000\n\nEpoch 167/1000, Train Loss: 0.00015523, Validation Loss: 0.00002643, Accuracy: 1.0000\n\nEpoch 168/1000, Train Loss: 0.00015332, Validation Loss: 0.00002596, Accuracy: 1.0000\n\nEpoch 169/1000, Train Loss: 0.00015042, Validation Loss: 0.00002554, Accuracy: 1.0000\n\nEpoch 170/1000, Train Loss: 0.00014773, Validation Loss: 0.00002497, Accuracy: 1.0000\n\nEpoch 171/1000, Train Loss: 0.00014506, Validation Loss: 0.00002450, Accuracy: 1.0000\n\nEpoch 172/1000, Train Loss: 0.00014289, Validation Loss: 0.00002411, Accuracy: 1.0000\n\nEpoch 173/1000, Train Loss: 0.00014037, Validation Loss: 0.00002366, Accuracy: 1.0000\n\nEpoch 174/1000, Train Loss: 0.00013830, Validation Loss: 0.00002325, Accuracy: 1.0000\n\nEpoch 175/1000, Train Loss: 0.00013611, Validation Loss: 0.00002289, Accuracy: 1.0000\n\nEpoch 176/1000, Train Loss: 0.00013373, Validation Loss: 0.00002250, Accuracy: 1.0000\n\nEpoch 177/1000, Train Loss: 0.00013157, Validation Loss: 0.00002208, Accuracy: 1.0000\n\nEpoch 178/1000, Train Loss: 0.00012925, Validation Loss: 0.00002181, Accuracy: 1.0000\n\nEpoch 179/1000, Train Loss: 0.00012697, Validation Loss: 0.00002137, Accuracy: 1.0000\n\nEpoch 180/1000, Train Loss: 0.00012535, Validation Loss: 0.00002092, Accuracy: 1.0000\n\nEpoch 181/1000, Train Loss: 0.00012304, Validation Loss: 0.00002059, Accuracy: 1.0000\n\nEpoch 182/1000, Train Loss: 0.00012099, Validation Loss: 0.00002021, Accuracy: 1.0000\n\nEpoch 183/1000, Train Loss: 0.00011909, Validation Loss: 0.00001991, Accuracy: 1.0000\n\nEpoch 184/1000, Train Loss: 0.00011714, Validation Loss: 0.00001961, Accuracy: 1.0000\n\nEpoch 185/1000, Train Loss: 0.00011550, Validation Loss: 0.00001925, Accuracy: 1.0000\n\nEpoch 186/1000, Train Loss: 0.00011346, Validation Loss: 0.00001895, Accuracy: 1.0000\n\nEpoch 187/1000, Train Loss: 0.00011213, Validation Loss: 0.00001869, Accuracy: 1.0000\n\nEpoch 188/1000, Train Loss: 0.00010987, Validation Loss: 0.00001827, Accuracy: 1.0000\n\nEpoch 189/1000, Train Loss: 0.00010855, Validation Loss: 0.00001806, Accuracy: 1.0000\n\nEpoch 190/1000, Train Loss: 0.00010646, Validation Loss: 0.00001776, Accuracy: 1.0000\n\nEpoch 191/1000, Train Loss: 0.00010498, Validation Loss: 0.00001737, Accuracy: 1.0000\n\nEpoch 192/1000, Train Loss: 0.00010328, Validation Loss: 0.00001720, Accuracy: 1.0000\n\nEpoch 193/1000, Train Loss: 0.00010185, Validation Loss: 0.00001684, Accuracy: 1.0000\n\nEpoch 194/1000, Train Loss: 0.00010010, Validation Loss: 0.00001663, Accuracy: 1.0000\n\nEpoch 195/1000, Train Loss: 0.00009877, Validation Loss: 0.00001633, Accuracy: 1.0000\n\nEpoch 196/1000, Train Loss: 0.00009714, Validation Loss: 0.00001609, Accuracy: 1.0000\n\nEpoch 197/1000, Train Loss: 0.00009569, Validation Loss: 0.00001579, Accuracy: 1.0000\n\nEpoch 198/1000, Train Loss: 0.00009441, Validation Loss: 0.00001550, Accuracy: 1.0000\n\nEpoch 199/1000, Train Loss: 0.00009292, Validation Loss: 0.00001535, Accuracy: 1.0000\n\nEpoch 200/1000, Train Loss: 0.00009152, Validation Loss: 0.00001508, Accuracy: 1.0000\n\nEpoch 201/1000, Train Loss: 0.00009014, Validation Loss: 0.00001478, Accuracy: 1.0000\n\nEpoch 202/1000, Train Loss: 0.00008861, Validation Loss: 0.00001457, Accuracy: 1.0000\n\nEpoch 203/1000, Train Loss: 0.00008737, Validation Loss: 0.00001433, Accuracy: 1.0000\n\nEpoch 204/1000, Train Loss: 0.00008600, Validation Loss: 0.00001410, Accuracy: 1.0000\n\nEpoch 205/1000, Train Loss: 0.00008503, Validation Loss: 0.00001386, Accuracy: 1.0000\n\nEpoch 206/1000, Train Loss: 0.00008371, Validation Loss: 0.00001368, Accuracy: 1.0000\n\nEpoch 207/1000, Train Loss: 0.00008236, Validation Loss: 0.00001344, Accuracy: 1.0000\n\nEpoch 208/1000, Train Loss: 0.00008125, Validation Loss: 0.00001332, Accuracy: 1.0000\n\nEpoch 209/1000, Train Loss: 0.00007990, Validation Loss: 0.00001302, Accuracy: 1.0000\n\nEpoch 210/1000, Train Loss: 0.00007900, Validation Loss: 0.00001287, Accuracy: 1.0000\n\nEpoch 211/1000, Train Loss: 0.00007756, Validation Loss: 0.00001264, Accuracy: 1.0000\n\nEpoch 212/1000, Train Loss: 0.00007659, Validation Loss: 0.00001249, Accuracy: 1.0000\n\nEpoch 213/1000, Train Loss: 0.00007537, Validation Loss: 0.00001228, Accuracy: 1.0000\n\nEpoch 214/1000, Train Loss: 0.00007441, Validation Loss: 0.00001207, Accuracy: 1.0000\n\nEpoch 215/1000, Train Loss: 0.00007327, Validation Loss: 0.00001189, Accuracy: 1.0000\n\nEpoch 216/1000, Train Loss: 0.00007231, Validation Loss: 0.00001177, Accuracy: 1.0000\n\nEpoch 217/1000, Train Loss: 0.00007124, Validation Loss: 0.00001153, Accuracy: 1.0000\n\nEpoch 218/1000, Train Loss: 0.00007020, Validation Loss: 0.00001141, Accuracy: 1.0000\n\nEpoch 219/1000, Train Loss: 0.00006924, Validation Loss: 0.00001121, Accuracy: 1.0000\n\nEpoch 220/1000, Train Loss: 0.00006825, Validation Loss: 0.00001103, Accuracy: 1.0000\n\nEpoch 221/1000, Train Loss: 0.00006746, Validation Loss: 0.00001091, Accuracy: 1.0000\n\nEpoch 222/1000, Train Loss: 0.00006633, Validation Loss: 0.00001067, Accuracy: 1.0000\n\nEpoch 223/1000, Train Loss: 0.00006543, Validation Loss: 0.00001055, Accuracy: 1.0000\n\nEpoch 224/1000, Train Loss: 0.00006451, Validation Loss: 0.00001037, Accuracy: 1.0000\n\nEpoch 225/1000, Train Loss: 0.00006359, Validation Loss: 0.00001025, Accuracy: 1.0000\n\nEpoch 226/1000, Train Loss: 0.00006284, Validation Loss: 0.00001010, Accuracy: 1.0000\n\nEpoch 227/1000, Train Loss: 0.00006190, Validation Loss: 0.00000992, Accuracy: 1.0000\n\nEpoch 228/1000, Train Loss: 0.00006102, Validation Loss: 0.00000978, Accuracy: 1.0000\n\nEpoch 229/1000, Train Loss: 0.00006020, Validation Loss: 0.00000963, Accuracy: 1.0000\n\nEpoch 230/1000, Train Loss: 0.00005933, Validation Loss: 0.00000951, Accuracy: 1.0000\n\nEpoch 231/1000, Train Loss: 0.00005867, Validation Loss: 0.00000942, Accuracy: 1.0000\n\nEpoch 232/1000, Train Loss: 0.00005781, Validation Loss: 0.00000921, Accuracy: 1.0000\n\nEpoch 233/1000, Train Loss: 0.00005694, Validation Loss: 0.00000909, Accuracy: 1.0000\n\nEpoch 234/1000, Train Loss: 0.00005640, Validation Loss: 0.00000903, Accuracy: 1.0000\n\nEpoch 235/1000, Train Loss: 0.00005549, Validation Loss: 0.00000885, Accuracy: 1.0000\n\nEpoch 236/1000, Train Loss: 0.00005467, Validation Loss: 0.00000873, Accuracy: 1.0000\n\nEpoch 237/1000, Train Loss: 0.00005400, Validation Loss: 0.00000861, Accuracy: 1.0000\n\nEpoch 238/1000, Train Loss: 0.00005318, Validation Loss: 0.00000846, Accuracy: 1.0000\n\nEpoch 239/1000, Train Loss: 0.00005254, Validation Loss: 0.00000831, Accuracy: 1.0000\n\nEpoch 240/1000, Train Loss: 0.00005195, Validation Loss: 0.00000820, Accuracy: 1.0000\n\nEpoch 241/1000, Train Loss: 0.00005115, Validation Loss: 0.00000811, Accuracy: 1.0000\n\nEpoch 242/1000, Train Loss: 0.00005042, Validation Loss: 0.00000799, Accuracy: 1.0000\n\nEpoch 243/1000, Train Loss: 0.00004983, Validation Loss: 0.00000784, Accuracy: 1.0000\n\nEpoch 244/1000, Train Loss: 0.00004911, Validation Loss: 0.00000778, Accuracy: 1.0000\n\nEpoch 245/1000, Train Loss: 0.00004849, Validation Loss: 0.00000766, Accuracy: 1.0000\n\nEpoch 246/1000, Train Loss: 0.00004778, Validation Loss: 0.00000760, Accuracy: 1.0000\n\nEpoch 247/1000, Train Loss: 0.00004732, Validation Loss: 0.00000748, Accuracy: 1.0000\n\nEpoch 248/1000, Train Loss: 0.00004665, Validation Loss: 0.00000736, Accuracy: 1.0000\n\nEpoch 249/1000, Train Loss: 0.00004596, Validation Loss: 0.00000721, Accuracy: 1.0000\n\nEpoch 250/1000, Train Loss: 0.00004540, Validation Loss: 0.00000709, Accuracy: 1.0000\n\nEpoch 251/1000, Train Loss: 0.00004477, Validation Loss: 0.00000703, Accuracy: 1.0000\n\nEpoch 252/1000, Train Loss: 0.00004420, Validation Loss: 0.00000691, Accuracy: 1.0000\n\nEpoch 253/1000, Train Loss: 0.00004369, Validation Loss: 0.00000682, Accuracy: 1.0000\n\nEpoch 254/1000, Train Loss: 0.00004313, Validation Loss: 0.00000671, Accuracy: 1.0000\n\nEpoch 255/1000, Train Loss: 0.00004254, Validation Loss: 0.00000662, Accuracy: 1.0000\n\nEpoch 256/1000, Train Loss: 0.00004201, Validation Loss: 0.00000656, Accuracy: 1.0000\n\nEpoch 257/1000, Train Loss: 0.00004144, Validation Loss: 0.00000644, Accuracy: 1.0000\n\nEpoch 258/1000, Train Loss: 0.00004093, Validation Loss: 0.00000638, Accuracy: 1.0000\n\nEpoch 259/1000, Train Loss: 0.00004040, Validation Loss: 0.00000626, Accuracy: 1.0000\n\nEpoch 260/1000, Train Loss: 0.00003989, Validation Loss: 0.00000620, Accuracy: 1.0000\n\nEpoch 261/1000, Train Loss: 0.00003950, Validation Loss: 0.00000611, Accuracy: 1.0000\n\nEpoch 262/1000, Train Loss: 0.00003888, Validation Loss: 0.00000602, Accuracy: 1.0000\n\nEpoch 263/1000, Train Loss: 0.00003839, Validation Loss: 0.00000596, Accuracy: 1.0000\n\nEpoch 264/1000, Train Loss: 0.00003794, Validation Loss: 0.00000587, Accuracy: 1.0000\n\nEpoch 265/1000, Train Loss: 0.00003751, Validation Loss: 0.00000581, Accuracy: 1.0000\n\nEpoch 266/1000, Train Loss: 0.00003697, Validation Loss: 0.00000572, Accuracy: 1.0000\n\nEpoch 267/1000, Train Loss: 0.00003650, Validation Loss: 0.00000563, Accuracy: 1.0000\n\nEpoch 268/1000, Train Loss: 0.00003609, Validation Loss: 0.00000557, Accuracy: 1.0000\n\nEpoch 269/1000, Train Loss: 0.00003558, Validation Loss: 0.00000551, Accuracy: 1.0000\n\nEpoch 270/1000, Train Loss: 0.00003515, Validation Loss: 0.00000542, Accuracy: 1.0000\n\nEpoch 271/1000, Train Loss: 0.00003472, Validation Loss: 0.00000539, Accuracy: 1.0000\n\nEpoch 272/1000, Train Loss: 0.00003425, Validation Loss: 0.00000527, Accuracy: 1.0000\n\nEpoch 273/1000, Train Loss: 0.00003387, Validation Loss: 0.00000522, Accuracy: 1.0000\n\nEpoch 274/1000, Train Loss: 0.00003342, Validation Loss: 0.00000516, Accuracy: 1.0000\n\nEpoch 275/1000, Train Loss: 0.00003305, Validation Loss: 0.00000510, Accuracy: 1.0000\n\nEpoch 276/1000, Train Loss: 0.00003263, Validation Loss: 0.00000504, Accuracy: 1.0000\n\nEpoch 277/1000, Train Loss: 0.00003215, Validation Loss: 0.00000492, Accuracy: 1.0000\n\nEpoch 278/1000, Train Loss: 0.00003186, Validation Loss: 0.00000486, Accuracy: 1.0000\n\nEpoch 279/1000, Train Loss: 0.00003139, Validation Loss: 0.00000480, Accuracy: 1.0000\n\nEpoch 280/1000, Train Loss: 0.00003110, Validation Loss: 0.00000474, Accuracy: 1.0000\n\nEpoch 281/1000, Train Loss: 0.00003072, Validation Loss: 0.00000468, Accuracy: 1.0000\n\nEpoch 282/1000, Train Loss: 0.00003024, Validation Loss: 0.00000462, Accuracy: 1.0000\n\nEpoch 283/1000, Train Loss: 0.00002996, Validation Loss: 0.00000453, Accuracy: 1.0000\n\nEpoch 284/1000, Train Loss: 0.00002954, Validation Loss: 0.00000447, Accuracy: 1.0000\n\nEpoch 285/1000, Train Loss: 0.00002918, Validation Loss: 0.00000441, Accuracy: 1.0000\n\nEpoch 286/1000, Train Loss: 0.00002886, Validation Loss: 0.00000435, Accuracy: 1.0000\n\nEpoch 287/1000, Train Loss: 0.00002846, Validation Loss: 0.00000429, Accuracy: 1.0000\n\nEpoch 288/1000, Train Loss: 0.00002819, Validation Loss: 0.00000423, Accuracy: 1.0000\n\nEpoch 289/1000, Train Loss: 0.00002777, Validation Loss: 0.00000417, Accuracy: 1.0000\n\nEpoch 290/1000, Train Loss: 0.00002744, Validation Loss: 0.00000414, Accuracy: 1.0000\n\nEpoch 291/1000, Train Loss: 0.00002715, Validation Loss: 0.00000408, Accuracy: 1.0000\n\nEpoch 292/1000, Train Loss: 0.00002683, Validation Loss: 0.00000402, Accuracy: 1.0000\n\nEpoch 293/1000, Train Loss: 0.00002649, Validation Loss: 0.00000396, Accuracy: 1.0000\n\nEpoch 294/1000, Train Loss: 0.00002616, Validation Loss: 0.00000390, Accuracy: 1.0000\n\nEpoch 295/1000, Train Loss: 0.00002587, Validation Loss: 0.00000390, Accuracy: 1.0000\n\nEpoch 296/1000, Train Loss: 0.00002554, Validation Loss: 0.00000384, Accuracy: 1.0000\n\nEpoch 297/1000, Train Loss: 0.00002534, Validation Loss: 0.00000378, Accuracy: 1.0000\n\nEpoch 298/1000, Train Loss: 0.00002492, Validation Loss: 0.00000373, Accuracy: 1.0000\n\nEpoch 299/1000, Train Loss: 0.00002470, Validation Loss: 0.00000367, Accuracy: 1.0000\n\nEpoch 300/1000, Train Loss: 0.00002438, Validation Loss: 0.00000367, Accuracy: 1.0000\n\nEpoch 301/1000, Train Loss: 0.00002414, Validation Loss: 0.00000361, Accuracy: 1.0000\n\nEpoch 302/1000, Train Loss: 0.00002378, Validation Loss: 0.00000355, Accuracy: 1.0000\n\nEpoch 303/1000, Train Loss: 0.00002351, Validation Loss: 0.00000352, Accuracy: 1.0000\n\nEpoch 304/1000, Train Loss: 0.00002325, Validation Loss: 0.00000346, Accuracy: 1.0000\n\nEpoch 305/1000, Train Loss: 0.00002297, Validation Loss: 0.00000343, Accuracy: 1.0000\n\nEpoch 306/1000, Train Loss: 0.00002269, Validation Loss: 0.00000340, Accuracy: 1.0000\n\nEpoch 307/1000, Train Loss: 0.00002249, Validation Loss: 0.00000334, Accuracy: 1.0000\n\nEpoch 308/1000, Train Loss: 0.00002218, Validation Loss: 0.00000334, Accuracy: 1.0000\n\nEpoch 309/1000, Train Loss: 0.00002192, Validation Loss: 0.00000328, Accuracy: 1.0000\n\nEpoch 310/1000, Train Loss: 0.00002168, Validation Loss: 0.00000322, Accuracy: 1.0000\n\nEpoch 311/1000, Train Loss: 0.00002144, Validation Loss: 0.00000316, Accuracy: 1.0000\n\nEpoch 312/1000, Train Loss: 0.00002116, Validation Loss: 0.00000316, Accuracy: 1.0000\n\nEpoch 313/1000, Train Loss: 0.00002092, Validation Loss: 0.00000310, Accuracy: 1.0000\n\nEpoch 314/1000, Train Loss: 0.00002066, Validation Loss: 0.00000307, Accuracy: 1.0000\n\nEpoch 315/1000, Train Loss: 0.00002049, Validation Loss: 0.00000301, Accuracy: 1.0000\n\nEpoch 316/1000, Train Loss: 0.00002019, Validation Loss: 0.00000301, Accuracy: 1.0000\n\nEpoch 317/1000, Train Loss: 0.00001998, Validation Loss: 0.00000295, Accuracy: 1.0000\n\nEpoch 318/1000, Train Loss: 0.00001974, Validation Loss: 0.00000292, Accuracy: 1.0000\n\nEpoch 319/1000, Train Loss: 0.00001953, Validation Loss: 0.00000289, Accuracy: 1.0000\n\nEpoch 320/1000, Train Loss: 0.00001929, Validation Loss: 0.00000283, Accuracy: 1.0000\n\nEpoch 321/1000, Train Loss: 0.00001906, Validation Loss: 0.00000283, Accuracy: 1.0000\n\nEpoch 322/1000, Train Loss: 0.00001884, Validation Loss: 0.00000277, Accuracy: 1.0000\n\nEpoch 323/1000, Train Loss: 0.00001862, Validation Loss: 0.00000277, Accuracy: 1.0000\n\nEpoch 324/1000, Train Loss: 0.00001844, Validation Loss: 0.00000271, Accuracy: 1.0000\n\nEpoch 325/1000, Train Loss: 0.00001822, Validation Loss: 0.00000271, Accuracy: 1.0000\n\nEpoch 326/1000, Train Loss: 0.00001799, Validation Loss: 0.00000265, Accuracy: 1.0000\n\nEpoch 327/1000, Train Loss: 0.00001781, Validation Loss: 0.00000265, Accuracy: 1.0000\n\nEpoch 328/1000, Train Loss: 0.00001759, Validation Loss: 0.00000259, Accuracy: 1.0000\n\nEpoch 329/1000, Train Loss: 0.00001743, Validation Loss: 0.00000259, Accuracy: 1.0000\n\nEpoch 330/1000, Train Loss: 0.00001723, Validation Loss: 0.00000253, Accuracy: 1.0000\n\nEpoch 331/1000, Train Loss: 0.00001703, Validation Loss: 0.00000250, Accuracy: 1.0000\n\nEpoch 332/1000, Train Loss: 0.00001681, Validation Loss: 0.00000247, Accuracy: 1.0000\n\nEpoch 333/1000, Train Loss: 0.00001661, Validation Loss: 0.00000244, Accuracy: 1.0000\n\nEpoch 334/1000, Train Loss: 0.00001652, Validation Loss: 0.00000244, Accuracy: 1.0000\n\nEpoch 335/1000, Train Loss: 0.00001625, Validation Loss: 0.00000238, Accuracy: 1.0000\n\nEpoch 336/1000, Train Loss: 0.00001606, Validation Loss: 0.00000235, Accuracy: 1.0000\n\nEpoch 337/1000, Train Loss: 0.00001589, Validation Loss: 0.00000232, Accuracy: 1.0000\n\nEpoch 338/1000, Train Loss: 0.00001574, Validation Loss: 0.00000232, Accuracy: 1.0000\n\nEpoch 339/1000, Train Loss: 0.00001553, Validation Loss: 0.00000226, Accuracy: 1.0000\n\nEpoch 340/1000, Train Loss: 0.00001535, Validation Loss: 0.00000226, Accuracy: 1.0000\n\nEpoch 341/1000, Train Loss: 0.00001517, Validation Loss: 0.00000221, Accuracy: 1.0000\n\nEpoch 342/1000, Train Loss: 0.00001504, Validation Loss: 0.00000221, Accuracy: 1.0000\n\nEpoch 343/1000, Train Loss: 0.00001484, Validation Loss: 0.00000212, Accuracy: 1.0000\n\nEpoch 344/1000, Train Loss: 0.00001467, Validation Loss: 0.00000212, Accuracy: 1.0000\n\nEpoch 345/1000, Train Loss: 0.00001455, Validation Loss: 0.00000209, Accuracy: 1.0000\n\nEpoch 346/1000, Train Loss: 0.00001435, Validation Loss: 0.00000203, Accuracy: 1.0000\n\nEpoch 347/1000, Train Loss: 0.00001421, Validation Loss: 0.00000203, Accuracy: 1.0000\n\nEpoch 348/1000, Train Loss: 0.00001407, Validation Loss: 0.00000200, Accuracy: 1.0000\n\nEpoch 349/1000, Train Loss: 0.00001388, Validation Loss: 0.00000197, Accuracy: 1.0000\n\nEpoch 350/1000, Train Loss: 0.00001376, Validation Loss: 0.00000194, Accuracy: 1.0000\n\nEpoch 351/1000, Train Loss: 0.00001362, Validation Loss: 0.00000194, Accuracy: 1.0000\n\nEpoch 352/1000, Train Loss: 0.00001343, Validation Loss: 0.00000191, Accuracy: 1.0000\n\nEpoch 353/1000, Train Loss: 0.00001328, Validation Loss: 0.00000188, Accuracy: 1.0000\n\nEpoch 354/1000, Train Loss: 0.00001313, Validation Loss: 0.00000185, Accuracy: 1.0000\n\nEpoch 355/1000, Train Loss: 0.00001297, Validation Loss: 0.00000182, Accuracy: 1.0000\n\nEpoch 356/1000, Train Loss: 0.00001285, Validation Loss: 0.00000182, Accuracy: 1.0000\n\nEpoch 357/1000, Train Loss: 0.00001269, Validation Loss: 0.00000179, Accuracy: 1.0000\n\nEpoch 358/1000, Train Loss: 0.00001255, Validation Loss: 0.00000176, Accuracy: 1.0000\n\nEpoch 359/1000, Train Loss: 0.00001248, Validation Loss: 0.00000176, Accuracy: 1.0000\n\nEpoch 360/1000, Train Loss: 0.00001230, Validation Loss: 0.00000170, Accuracy: 1.0000\n\nEpoch 361/1000, Train Loss: 0.00001219, Validation Loss: 0.00000170, Accuracy: 1.0000\n\nEpoch 362/1000, Train Loss: 0.00001203, Validation Loss: 0.00000170, Accuracy: 1.0000\n\nEpoch 363/1000, Train Loss: 0.00001191, Validation Loss: 0.00000164, Accuracy: 1.0000\n\nEpoch 364/1000, Train Loss: 0.00001176, Validation Loss: 0.00000164, Accuracy: 1.0000\n\nEpoch 365/1000, Train Loss: 0.00001163, Validation Loss: 0.00000164, Accuracy: 1.0000\n\nEpoch 366/1000, Train Loss: 0.00001153, Validation Loss: 0.00000161, Accuracy: 1.0000\n\nEpoch 367/1000, Train Loss: 0.00001137, Validation Loss: 0.00000158, Accuracy: 1.0000\n\nEpoch 368/1000, Train Loss: 0.00001126, Validation Loss: 0.00000158, Accuracy: 1.0000\n\nEpoch 369/1000, Train Loss: 0.00001116, Validation Loss: 0.00000155, Accuracy: 1.0000\n\nEpoch 370/1000, Train Loss: 0.00001102, Validation Loss: 0.00000152, Accuracy: 1.0000\n\nEpoch 371/1000, Train Loss: 0.00001091, Validation Loss: 0.00000152, Accuracy: 1.0000\n\nEpoch 372/1000, Train Loss: 0.00001080, Validation Loss: 0.00000149, Accuracy: 1.0000\n\nEpoch 373/1000, Train Loss: 0.00001066, Validation Loss: 0.00000149, Accuracy: 1.0000\n\nEpoch 374/1000, Train Loss: 0.00001055, Validation Loss: 0.00000146, Accuracy: 1.0000\n\nEpoch 375/1000, Train Loss: 0.00001043, Validation Loss: 0.00000146, Accuracy: 1.0000\n\nEpoch 376/1000, Train Loss: 0.00001032, Validation Loss: 0.00000143, Accuracy: 1.0000\n\nEpoch 377/1000, Train Loss: 0.00001023, Validation Loss: 0.00000140, Accuracy: 1.0000\n\nEpoch 378/1000, Train Loss: 0.00001013, Validation Loss: 0.00000140, Accuracy: 1.0000\n\nEpoch 379/1000, Train Loss: 0.00000998, Validation Loss: 0.00000137, Accuracy: 1.0000\n\nEpoch 380/1000, Train Loss: 0.00000991, Validation Loss: 0.00000137, Accuracy: 1.0000\n\nEpoch 381/1000, Train Loss: 0.00000978, Validation Loss: 0.00000137, Accuracy: 1.0000\n\nEpoch 382/1000, Train Loss: 0.00000971, Validation Loss: 0.00000134, Accuracy: 1.0000\n\nEpoch 383/1000, Train Loss: 0.00000958, Validation Loss: 0.00000131, Accuracy: 1.0000\n\nEpoch 384/1000, Train Loss: 0.00000947, Validation Loss: 0.00000131, Accuracy: 1.0000\n\nEpoch 385/1000, Train Loss: 0.00000937, Validation Loss: 0.00000131, Accuracy: 1.0000\n\nEpoch 386/1000, Train Loss: 0.00000931, Validation Loss: 0.00000125, Accuracy: 1.0000\n\nEpoch 387/1000, Train Loss: 0.00000918, Validation Loss: 0.00000125, Accuracy: 1.0000\n\nEpoch 388/1000, Train Loss: 0.00000908, Validation Loss: 0.00000125, Accuracy: 1.0000\n\nEpoch 389/1000, Train Loss: 0.00000899, Validation Loss: 0.00000125, Accuracy: 1.0000\n\nEpoch 390/1000, Train Loss: 0.00000890, Validation Loss: 0.00000119, Accuracy: 1.0000\n\nEpoch 391/1000, Train Loss: 0.00000879, Validation Loss: 0.00000119, Accuracy: 1.0000\n\nEpoch 392/1000, Train Loss: 0.00000871, Validation Loss: 0.00000119, Accuracy: 1.0000\n\nEpoch 393/1000, Train Loss: 0.00000861, Validation Loss: 0.00000119, Accuracy: 1.0000\n\nEpoch 394/1000, Train Loss: 0.00000852, Validation Loss: 0.00000116, Accuracy: 1.0000\n\nEpoch 395/1000, Train Loss: 0.00000842, Validation Loss: 0.00000113, Accuracy: 1.0000\n\nEpoch 396/1000, Train Loss: 0.00000834, Validation Loss: 0.00000113, Accuracy: 1.0000\n\nEpoch 397/1000, Train Loss: 0.00000824, Validation Loss: 0.00000113, Accuracy: 1.0000\n\nEpoch 398/1000, Train Loss: 0.00000816, Validation Loss: 0.00000110, Accuracy: 1.0000\n\nEpoch 399/1000, Train Loss: 0.00000808, Validation Loss: 0.00000110, Accuracy: 1.0000\n\nEpoch 400/1000, Train Loss: 0.00000799, Validation Loss: 0.00000107, Accuracy: 1.0000\n\nEpoch 401/1000, Train Loss: 0.00000793, Validation Loss: 0.00000107, Accuracy: 1.0000\n\nEpoch 402/1000, Train Loss: 0.00000783, Validation Loss: 0.00000107, Accuracy: 1.0000\n\nEpoch 403/1000, Train Loss: 0.00000774, Validation Loss: 0.00000104, Accuracy: 1.0000\n\nEpoch 404/1000, Train Loss: 0.00000765, Validation Loss: 0.00000104, Accuracy: 1.0000\n\nEpoch 405/1000, Train Loss: 0.00000758, Validation Loss: 0.00000104, Accuracy: 1.0000\n\nEpoch 406/1000, Train Loss: 0.00000750, Validation Loss: 0.00000101, Accuracy: 1.0000\n\nEpoch 407/1000, Train Loss: 0.00000742, Validation Loss: 0.00000101, Accuracy: 1.0000\n\nEpoch 408/1000, Train Loss: 0.00000734, Validation Loss: 0.00000098, Accuracy: 1.0000\n\nEpoch 409/1000, Train Loss: 0.00000729, Validation Loss: 0.00000098, Accuracy: 1.0000\n\nEpoch 410/1000, Train Loss: 0.00000720, Validation Loss: 0.00000098, Accuracy: 1.0000\n\nEpoch 411/1000, Train Loss: 0.00000711, Validation Loss: 0.00000098, Accuracy: 1.0000\n\nEpoch 412/1000, Train Loss: 0.00000704, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 413/1000, Train Loss: 0.00000699, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 414/1000, Train Loss: 0.00000691, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 415/1000, Train Loss: 0.00000684, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 416/1000, Train Loss: 0.00000676, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 417/1000, Train Loss: 0.00000669, Validation Loss: 0.00000092, Accuracy: 1.0000\n\nEpoch 418/1000, Train Loss: 0.00000662, Validation Loss: 0.00000086, Accuracy: 1.0000\n\nEpoch 419/1000, Train Loss: 0.00000655, Validation Loss: 0.00000086, Accuracy: 1.0000\n\nEpoch 420/1000, Train Loss: 0.00000649, Validation Loss: 0.00000086, Accuracy: 1.0000\n\nEpoch 421/1000, Train Loss: 0.00000641, Validation Loss: 0.00000086, Accuracy: 1.0000\n\nEpoch 422/1000, Train Loss: 0.00000635, Validation Loss: 0.00000086, Accuracy: 1.0000\n\nEpoch 423/1000, Train Loss: 0.00000628, Validation Loss: 0.00000083, Accuracy: 1.0000\n\nEpoch 424/1000, Train Loss: 0.00000623, Validation Loss: 0.00000080, Accuracy: 1.0000\n\nEpoch 425/1000, Train Loss: 0.00000614, Validation Loss: 0.00000080, Accuracy: 1.0000\n\nEpoch 426/1000, Train Loss: 0.00000609, Validation Loss: 0.00000080, Accuracy: 1.0000\n\nEpoch 427/1000, Train Loss: 0.00000603, Validation Loss: 0.00000080, Accuracy: 1.0000\n\nEpoch 428/1000, Train Loss: 0.00000595, Validation Loss: 0.00000080, Accuracy: 1.0000\n\nEpoch 429/1000, Train Loss: 0.00000590, Validation Loss: 0.00000077, Accuracy: 1.0000\n\nEpoch 430/1000, Train Loss: 0.00000584, Validation Loss: 0.00000077, Accuracy: 1.0000\n\nEpoch 431/1000, Train Loss: 0.00000577, Validation Loss: 0.00000077, Accuracy: 1.0000\n\nEpoch 432/1000, Train Loss: 0.00000572, Validation Loss: 0.00000075, Accuracy: 1.0000\n\nEpoch 433/1000, Train Loss: 0.00000568, Validation Loss: 0.00000075, Accuracy: 1.0000\n\nEpoch 434/1000, Train Loss: 0.00000560, Validation Loss: 0.00000075, Accuracy: 1.0000\n\nEpoch 435/1000, Train Loss: 0.00000554, Validation Loss: 0.00000075, Accuracy: 1.0000\n\nEpoch 436/1000, Train Loss: 0.00000549, Validation Loss: 0.00000072, Accuracy: 1.0000\n\nEpoch 437/1000, Train Loss: 0.00000542, Validation Loss: 0.00000072, Accuracy: 1.0000\n\nEpoch 438/1000, Train Loss: 0.00000537, Validation Loss: 0.00000072, Accuracy: 1.0000\n\nEpoch 439/1000, Train Loss: 0.00000533, Validation Loss: 0.00000072, Accuracy: 1.0000\n\nEpoch 440/1000, Train Loss: 0.00000527, Validation Loss: 0.00000069, Accuracy: 1.0000\n\nEpoch 441/1000, Train Loss: 0.00000523, Validation Loss: 0.00000069, Accuracy: 1.0000\n\nEpoch 442/1000, Train Loss: 0.00000517, Validation Loss: 0.00000069, Accuracy: 1.0000\n\nEpoch 443/1000, Train Loss: 0.00000510, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 444/1000, Train Loss: 0.00000506, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 445/1000, Train Loss: 0.00000501, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 446/1000, Train Loss: 0.00000497, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 447/1000, Train Loss: 0.00000490, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 448/1000, Train Loss: 0.00000486, Validation Loss: 0.00000066, Accuracy: 1.0000\n\nEpoch 449/1000, Train Loss: 0.00000481, Validation Loss: 0.00000063, Accuracy: 1.0000\n\nEpoch 450/1000, Train Loss: 0.00000475, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 451/1000, Train Loss: 0.00000471, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 452/1000, Train Loss: 0.00000466, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 453/1000, Train Loss: 0.00000461, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 454/1000, Train Loss: 0.00000457, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 455/1000, Train Loss: 0.00000451, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 456/1000, Train Loss: 0.00000447, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 457/1000, Train Loss: 0.00000443, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 458/1000, Train Loss: 0.00000439, Validation Loss: 0.00000060, Accuracy: 1.0000\n\nEpoch 459/1000, Train Loss: 0.00000435, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 460/1000, Train Loss: 0.00000430, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 461/1000, Train Loss: 0.00000425, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 462/1000, Train Loss: 0.00000421, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 463/1000, Train Loss: 0.00000417, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 464/1000, Train Loss: 0.00000412, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 465/1000, Train Loss: 0.00000409, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 466/1000, Train Loss: 0.00000405, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 467/1000, Train Loss: 0.00000400, Validation Loss: 0.00000054, Accuracy: 1.0000\n\nEpoch 468/1000, Train Loss: 0.00000396, Validation Loss: 0.00000051, Accuracy: 1.0000\n\nEpoch 469/1000, Train Loss: 0.00000393, Validation Loss: 0.00000051, Accuracy: 1.0000\n\nEpoch 470/1000, Train Loss: 0.00000389, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 471/1000, Train Loss: 0.00000384, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 472/1000, Train Loss: 0.00000381, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 473/1000, Train Loss: 0.00000377, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 474/1000, Train Loss: 0.00000373, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 475/1000, Train Loss: 0.00000370, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 476/1000, Train Loss: 0.00000365, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 477/1000, Train Loss: 0.00000362, Validation Loss: 0.00000048, Accuracy: 1.0000\n\nEpoch 478/1000, Train Loss: 0.00000359, Validation Loss: 0.00000045, Accuracy: 1.0000\n\nEpoch 479/1000, Train Loss: 0.00000356, Validation Loss: 0.00000045, Accuracy: 1.0000\n\nEpoch 480/1000, Train Loss: 0.00000351, Validation Loss: 0.00000045, Accuracy: 1.0000\n\nEpoch 481/1000, Train Loss: 0.00000349, Validation Loss: 0.00000045, Accuracy: 1.0000\n\nEpoch 482/1000, Train Loss: 0.00000345, Validation Loss: 0.00000045, Accuracy: 1.0000\n\nEpoch 483/1000, Train Loss: 0.00000341, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 484/1000, Train Loss: 0.00000337, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 485/1000, Train Loss: 0.00000333, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 486/1000, Train Loss: 0.00000332, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 487/1000, Train Loss: 0.00000327, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 488/1000, Train Loss: 0.00000325, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 489/1000, Train Loss: 0.00000321, Validation Loss: 0.00000042, Accuracy: 1.0000\n\nEpoch 490/1000, Train Loss: 0.00000318, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 491/1000, Train Loss: 0.00000314, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 492/1000, Train Loss: 0.00000311, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 493/1000, Train Loss: 0.00000308, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 494/1000, Train Loss: 0.00000305, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 495/1000, Train Loss: 0.00000302, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 496/1000, Train Loss: 0.00000299, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 497/1000, Train Loss: 0.00000296, Validation Loss: 0.00000039, Accuracy: 1.0000\n\nEpoch 498/1000, Train Loss: 0.00000294, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 499/1000, Train Loss: 0.00000290, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 500/1000, Train Loss: 0.00000288, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 501/1000, Train Loss: 0.00000285, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 502/1000, Train Loss: 0.00000282, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 503/1000, Train Loss: 0.00000280, Validation Loss: 0.00000036, Accuracy: 1.0000\n\nEpoch 504/1000, Train Loss: 0.00000276, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 505/1000, Train Loss: 0.00000273, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 506/1000, Train Loss: 0.00000271, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 507/1000, Train Loss: 0.00000269, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 508/1000, Train Loss: 0.00000266, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 509/1000, Train Loss: 0.00000263, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 510/1000, Train Loss: 0.00000260, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 511/1000, Train Loss: 0.00000257, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 512/1000, Train Loss: 0.00000255, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 513/1000, Train Loss: 0.00000253, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 514/1000, Train Loss: 0.00000250, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 515/1000, Train Loss: 0.00000248, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 516/1000, Train Loss: 0.00000245, Validation Loss: 0.00000033, Accuracy: 1.0000\n\nEpoch 517/1000, Train Loss: 0.00000243, Validation Loss: 0.00000030, Accuracy: 1.0000\n\nEpoch 518/1000, Train Loss: 0.00000240, Validation Loss: 0.00000030, Accuracy: 1.0000\n\nEpoch 519/1000, Train Loss: 0.00000238, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 520/1000, Train Loss: 0.00000235, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 521/1000, Train Loss: 0.00000234, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 522/1000, Train Loss: 0.00000231, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 523/1000, Train Loss: 0.00000229, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 524/1000, Train Loss: 0.00000227, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 525/1000, Train Loss: 0.00000225, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 526/1000, Train Loss: 0.00000222, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 527/1000, Train Loss: 0.00000219, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 528/1000, Train Loss: 0.00000218, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 529/1000, Train Loss: 0.00000215, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 530/1000, Train Loss: 0.00000214, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 531/1000, Train Loss: 0.00000212, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 532/1000, Train Loss: 0.00000209, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 533/1000, Train Loss: 0.00000207, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 534/1000, Train Loss: 0.00000205, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 535/1000, Train Loss: 0.00000203, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 536/1000, Train Loss: 0.00000201, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 537/1000, Train Loss: 0.00000199, Validation Loss: 0.00000027, Accuracy: 1.0000\n\nEpoch 538/1000, Train Loss: 0.00000197, Validation Loss: 0.00000024, Accuracy: 1.0000\n\nEpoch 539/1000, Train Loss: 0.00000196, Validation Loss: 0.00000024, Accuracy: 1.0000\n\nEpoch 540/1000, Train Loss: 0.00000194, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 541/1000, Train Loss: 0.00000192, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 542/1000, Train Loss: 0.00000190, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 543/1000, Train Loss: 0.00000189, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 544/1000, Train Loss: 0.00000186, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 545/1000, Train Loss: 0.00000184, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 546/1000, Train Loss: 0.00000183, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 547/1000, Train Loss: 0.00000181, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 548/1000, Train Loss: 0.00000179, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 549/1000, Train Loss: 0.00000177, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 550/1000, Train Loss: 0.00000175, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 551/1000, Train Loss: 0.00000174, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 552/1000, Train Loss: 0.00000172, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 553/1000, Train Loss: 0.00000171, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 554/1000, Train Loss: 0.00000169, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 555/1000, Train Loss: 0.00000168, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 556/1000, Train Loss: 0.00000166, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 557/1000, Train Loss: 0.00000164, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 558/1000, Train Loss: 0.00000163, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 559/1000, Train Loss: 0.00000161, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 560/1000, Train Loss: 0.00000159, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 561/1000, Train Loss: 0.00000159, Validation Loss: 0.00000021, Accuracy: 1.0000\n\nEpoch 562/1000, Train Loss: 0.00000157, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 563/1000, Train Loss: 0.00000155, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 564/1000, Train Loss: 0.00000154, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 565/1000, Train Loss: 0.00000152, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 566/1000, Train Loss: 0.00000150, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 567/1000, Train Loss: 0.00000149, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 568/1000, Train Loss: 0.00000148, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 569/1000, Train Loss: 0.00000146, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 570/1000, Train Loss: 0.00000145, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 571/1000, Train Loss: 0.00000143, Validation Loss: 0.00000018, Accuracy: 1.0000\n\nEpoch 572/1000, Train Loss: 0.00000142, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 573/1000, Train Loss: 0.00000141, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 574/1000, Train Loss: 0.00000139, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 575/1000, Train Loss: 0.00000138, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 576/1000, Train Loss: 0.00000136, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 577/1000, Train Loss: 0.00000136, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 578/1000, Train Loss: 0.00000134, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 579/1000, Train Loss: 0.00000133, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 580/1000, Train Loss: 0.00000132, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 581/1000, Train Loss: 0.00000130, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 582/1000, Train Loss: 0.00000129, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 583/1000, Train Loss: 0.00000128, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 584/1000, Train Loss: 0.00000127, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 585/1000, Train Loss: 0.00000125, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 586/1000, Train Loss: 0.00000124, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 587/1000, Train Loss: 0.00000123, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 588/1000, Train Loss: 0.00000122, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 589/1000, Train Loss: 0.00000120, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 590/1000, Train Loss: 0.00000120, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 591/1000, Train Loss: 0.00000118, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 592/1000, Train Loss: 0.00000117, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 593/1000, Train Loss: 0.00000117, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 594/1000, Train Loss: 0.00000116, Validation Loss: 0.00000015, Accuracy: 1.0000\n\nEpoch 595/1000, Train Loss: 0.00000114, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 596/1000, Train Loss: 0.00000113, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 597/1000, Train Loss: 0.00000112, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 598/1000, Train Loss: 0.00000111, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 599/1000, Train Loss: 0.00000109, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 600/1000, Train Loss: 0.00000108, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 601/1000, Train Loss: 0.00000107, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 602/1000, Train Loss: 0.00000107, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 603/1000, Train Loss: 0.00000105, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 604/1000, Train Loss: 0.00000104, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 605/1000, Train Loss: 0.00000103, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 606/1000, Train Loss: 0.00000102, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 607/1000, Train Loss: 0.00000101, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 608/1000, Train Loss: 0.00000101, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 609/1000, Train Loss: 0.00000099, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 610/1000, Train Loss: 0.00000098, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 611/1000, Train Loss: 0.00000096, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 612/1000, Train Loss: 0.00000096, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 613/1000, Train Loss: 0.00000096, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 614/1000, Train Loss: 0.00000095, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 615/1000, Train Loss: 0.00000094, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 616/1000, Train Loss: 0.00000093, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 617/1000, Train Loss: 0.00000092, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 618/1000, Train Loss: 0.00000091, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 619/1000, Train Loss: 0.00000091, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 620/1000, Train Loss: 0.00000089, Validation Loss: 0.00000012, Accuracy: 1.0000\n\nEpoch 621/1000, Train Loss: 0.00000088, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 622/1000, Train Loss: 0.00000088, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 623/1000, Train Loss: 0.00000087, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 624/1000, Train Loss: 0.00000086, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 625/1000, Train Loss: 0.00000085, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 626/1000, Train Loss: 0.00000084, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 627/1000, Train Loss: 0.00000084, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 628/1000, Train Loss: 0.00000083, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 629/1000, Train Loss: 0.00000082, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 630/1000, Train Loss: 0.00000082, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 631/1000, Train Loss: 0.00000080, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 632/1000, Train Loss: 0.00000080, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 633/1000, Train Loss: 0.00000078, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 634/1000, Train Loss: 0.00000077, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 635/1000, Train Loss: 0.00000077, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 636/1000, Train Loss: 0.00000076, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 637/1000, Train Loss: 0.00000076, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 638/1000, Train Loss: 0.00000075, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 639/1000, Train Loss: 0.00000074, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 640/1000, Train Loss: 0.00000074, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 641/1000, Train Loss: 0.00000073, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 642/1000, Train Loss: 0.00000073, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 643/1000, Train Loss: 0.00000072, Validation Loss: 0.00000009, Accuracy: 1.0000\n\nEpoch 644/1000, Train Loss: 0.00000071, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 645/1000, Train Loss: 0.00000071, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 646/1000, Train Loss: 0.00000069, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 647/1000, Train Loss: 0.00000069, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 648/1000, Train Loss: 0.00000069, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 649/1000, Train Loss: 0.00000068, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 650/1000, Train Loss: 0.00000067, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 651/1000, Train Loss: 0.00000066, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 652/1000, Train Loss: 0.00000065, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 653/1000, Train Loss: 0.00000065, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 654/1000, Train Loss: 0.00000064, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 655/1000, Train Loss: 0.00000064, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 656/1000, Train Loss: 0.00000063, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 657/1000, Train Loss: 0.00000062, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 658/1000, Train Loss: 0.00000062, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 659/1000, Train Loss: 0.00000061, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 660/1000, Train Loss: 0.00000060, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 661/1000, Train Loss: 0.00000060, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 662/1000, Train Loss: 0.00000060, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 663/1000, Train Loss: 0.00000058, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 664/1000, Train Loss: 0.00000058, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 665/1000, Train Loss: 0.00000058, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 666/1000, Train Loss: 0.00000057, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 667/1000, Train Loss: 0.00000056, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 668/1000, Train Loss: 0.00000056, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 669/1000, Train Loss: 0.00000056, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 670/1000, Train Loss: 0.00000055, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 671/1000, Train Loss: 0.00000054, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 672/1000, Train Loss: 0.00000054, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 673/1000, Train Loss: 0.00000054, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 674/1000, Train Loss: 0.00000053, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 675/1000, Train Loss: 0.00000053, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 676/1000, Train Loss: 0.00000053, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 677/1000, Train Loss: 0.00000053, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 678/1000, Train Loss: 0.00000051, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 679/1000, Train Loss: 0.00000051, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 680/1000, Train Loss: 0.00000051, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 681/1000, Train Loss: 0.00000050, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 682/1000, Train Loss: 0.00000050, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 683/1000, Train Loss: 0.00000050, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 684/1000, Train Loss: 0.00000049, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 685/1000, Train Loss: 0.00000048, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 686/1000, Train Loss: 0.00000048, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 687/1000, Train Loss: 0.00000048, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 688/1000, Train Loss: 0.00000047, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 689/1000, Train Loss: 0.00000045, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 690/1000, Train Loss: 0.00000045, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 691/1000, Train Loss: 0.00000045, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 692/1000, Train Loss: 0.00000045, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 693/1000, Train Loss: 0.00000044, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEpoch 694/1000, Train Loss: 0.00000044, Validation Loss: 0.00000006, Accuracy: 1.0000\n\nEarly stopping\n"}]},{"cell_type":"code","source":"# 將模型設為評估模式\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\n\n# 預測\nwith torch.no_grad():\n    predictions = model(x_test_tensor)\n\n#print(predictions)\n# 取得預測的類別\npredicted_classes = torch.argmax(predictions, dim=1)\n#print(predicted_classes)\npredicted_classes = predicted_classes.numpy()\ny_test = np.array(y_test)\n#print(y_test)\naccuracy = accuracy_score(y_test, predicted_classes)\nprint(\"預測準確率：\", accuracy)\n\n# 輸出預測結果\n#print(predicted_classes)\n\n# 計算混淆矩陣\ncm = confusion_matrix(y_test, predicted_classes, labels=[0, 1])\n# 將混淆矩陣轉換成百分比\n#cm_percentage = cm / cm.sum(axis=1, keepdims=True) * 100\n\nprint(\"混淆矩陣:\")\nprint(cm)","metadata":{},"execution_count":41,"outputs":[{"name":"stdout","output_type":"stream","text":"預測準確率： 1.0\n\n混淆矩陣:\n\n[[0 0]\n\n [0 2]]\n"}]},{"cell_type":"code","source":"news_data=clean_data[-10:]\n\n\n# 去除字符串列名，并将剩余数值转换为 tensor\nnews_data_values = news_data.values\nnews_data_tensor = torch.tensor(news_data_values, dtype=torch.float32)\n\nprint(news_data_tensor.shape)\n#news_data_tensor = torch.stack(news_data_tensors)\nnews_data_tensor = news_data_tensor.unsqueeze(0).repeat(2, 1, 1)\n\nwith torch.no_grad():\n    results = model(news_data_tensor)\n    \nresults_classes = torch.argmax(results, dim=1)\n","metadata":{},"execution_count":72,"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([10, 7])\n\ntensor(1)\n"}]},{"cell_type":"code","source":"if results_classes[0]==1:\n    msg='可以進場，10個交易日後會賺。'\nelse:\n    msg='不要進場。'\n","metadata":{},"execution_count":74,"outputs":[{"name":"stdout","output_type":"stream","text":"可以進場，10個交易日後會賺。\n"}]},{"cell_type":"code","source":"import requests\n\ndef send_message(msg, image_path):\n    headers = {\n        'Authorization': 'Bearer uTRd8WZAkORjjq5AYTwWqSCx7MuKMrzX9N9FiqkehsX'\n    }\n\n    payload = {\n        'message': msg\n    }\n    files = {\n        'imageFile': open(image_path, 'rb')\n    }\n\n    r = requests.post(\"https://notify-api.line.me/api/notify\", headers=headers, data=payload, files=files)\n    return r.status_code, r.json()","metadata":{},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# 訊息和圖片路徑\n#msg = 'test'\n#imgfile = r'C:/Users/Dominic/Desktop/小說集/家裡蹲妹妹竟然要當冒險者/03447-3233128412-blue cloak.png'\n\nstatus_code, response_data = send_message(msg)\nprint(f\"狀態碼: {status_code}\")","metadata":{},"execution_count":76,"outputs":[{"name":"stdout","output_type":"stream","text":"狀態碼: 200\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}