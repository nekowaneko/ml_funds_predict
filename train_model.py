import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
from . import config
from . import utils

# å®šç¾© TCN æ¨¡å‹
# Input shape: (Batch, Window_Size, Features)
class TCN(nn.Module):
    def __init__(self, input_size, output_size):
        super(TCN, self).__init__()
        # input_size é€™è£¡å°æ‡‰ config.WINDOW_SIZE
        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2)
        
        # ç¶“é Conv1d å’Œ MaxPool1d å¾Œçš„ç¶­åº¦è¨ˆç®—:
        # Input: (N, 10, 7)
        # Conv1d: (N, 64, 7) (padding=1, kernel=3 ç¶­æŒé•·åº¦)
        # MaxPool1d: (N, 64, 3) (7 // 2 = 3)
        # Flatten: 64 * 3 = 192 (æ³¨æ„ï¼šé€™å–æ±ºæ–¼ Feature æ•¸é‡ï¼Œé€™è£¡å‡è¨­ç‚º 7)
        self.fc1 = nn.Linear(192, 128)
        self.fc2 = nn.Linear(128, output_size)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1) # å±•å¹³ (Flatten)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def preprocess_data(df):
    """
    è³‡æ–™å‰è™•ç†ï¼šæ¸…æ´—èˆ‡æ¨™æº–åŒ–
    
    åƒæ•¸:
        df (pd.DataFrame): åŸå§‹è‚¡ç¥¨è³‡æ–™
        
    å›å‚³:
        pd.DataFrame: è™•ç†å¾Œçš„è³‡æ–™
    """
    # æ’é™¤ä¸éœ€è¦çš„æ¬„ä½
    columns_to_exclude = ['index', 'date', 'change', 'æ—¥æœŸ', 'æ¼²è·Œ', 'æ¼²è·Œå¹…(%)']
    # åªä¿ç•™å­˜åœ¨æ–¼ df çš„æ¬„ä½
    cols_to_drop = [c for c in columns_to_exclude if c in df.columns]
    data_to_normalize = df.drop(columns=cols_to_drop)
    
    # ç¢ºä¿æ•¸å€¼å‹åˆ¥ (ç§»é™¤é€—è™Ÿä¸¦è½‰ç‚ºæµ®é»æ•¸)
    for column in data_to_normalize.columns:
        if data_to_normalize[column].dtype == 'object':
             data_to_normalize[column] = data_to_normalize[column].str.replace(',', '').astype(float)
        else:
             data_to_normalize[column] = data_to_normalize[column].astype(float)
             
    # æ¨™æº–åŒ–è™•ç† (Features)
    scaler = StandardScaler()
    normalized_data = scaler.fit_transform(data_to_normalize)
    clean_data = pd.DataFrame(normalized_data, columns=data_to_normalize.columns)
    
    return clean_data

def create_labels(df, window_size):
    """
    å»ºç«‹æ¨™ç±¤ï¼šæ¯”è¼ƒç•¶å‰æ·¨å€¼èˆ‡æœªä¾†æ·¨å€¼
    
    åƒæ•¸:
        df (pd.DataFrame): åŸå§‹è‚¡ç¥¨è³‡æ–™
        window_size (int): é æ¸¬çš„æ™‚é–“è·¨åº¦
        
    å›å‚³:
        list: æ¨™ç±¤åˆ—è¡¨ (1: ä¸Šæ¼², 0: ä¸‹è·Œæˆ–æŒå¹³)
    """
    # é–å®šæ”¶ç›¤åƒ¹æ¬„ä½
    target_col = 'close' if 'close' in df.columns else 'æ”¶ç›¤åƒ¹'
    
    prices = None
    if target_col in df.columns:
         if df[target_col].dtype == 'object':
            prices = df[target_col].str.replace(',', '').astype(float).values
         else:
            prices = df[target_col].values
    else:
        # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¢ºçš„æ”¶ç›¤åƒ¹æ¬„ä½ï¼Œå˜—è©¦ä½¿ç”¨æœ€å¾Œä¸€æ¬„
        prices = df.iloc[:, -1].values

    labels = []
    length = len(prices)
    
    # ç”¢ç”Ÿæ¨™ç±¤ï¼šå¦‚æœ N å¤©å¾Œçš„åƒ¹æ ¼ > ç•¶å‰åƒ¹æ ¼ï¼Œå‰‡æ¨™è¨˜ç‚º 1
    for i in range(length - window_size):
        if prices[i + window_size] > prices[i]:
            labels.append(1)
        else:
            labels.append(0)
            
    return labels

def train_model(stock_code):
    """
    è¨“ç·´æŒ‡å®šè‚¡ç¥¨çš„é æ¸¬æ¨¡å‹
    """
    print(f"ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹: {stock_code}")
    
    file_name = f"{stock_code}{config.SYMBOL_DICT.get(stock_code, '')}.csv"
    file_path = os.path.join(config.DATA_DIR, file_name)
    
    # 1. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™æª”ï¼š{file_path}ï¼Œè·³éè¨“ç·´ã€‚")
        return

    try:
        # è®€å– CSV
        df = pd.read_csv(file_path)
        
        # ç°¡å–®æ˜ å°„æ¬„ä½åç¨± (å¦‚æœ CSV æ²’æœ‰ Header)
        if len(df.columns) == 10:
             df.columns = ['index', 'date', 'volume', 'amount', 'open', 'high', 'low', 'close', 'change', 'transactions']
        
        # 2. æª¢æŸ¥è³‡æ–™é‡æ˜¯å¦è¶³å¤ 
        if len(df) < 50:
            print(f"âš ï¸ è³‡æ–™ä¸è¶³ ({len(df)} ç­†)ï¼Œè·³éè¨“ç·´ã€‚")
            return
            
        # è³‡æ–™å‰è™•ç†
        clean_data = preprocess_data(df)
        
        # æº–å‚™è¨“ç·´è³‡æ–™ (Sliding Window)
        window_size = config.WINDOW_SIZE
        x_data = []
        labels = create_labels(df, window_size) 
        
        # ç¢ºä¿è³‡æ–™é•·åº¦ä¸€è‡´
        valid_length = len(labels)
        
        if valid_length < window_size:
             print("âš ï¸ æœ‰æ•ˆè³‡æ–™é•·åº¦ä¸è¶³ä»¥å»ºç«‹ Windowï¼Œè·³éã€‚")
             return

        for i in range(valid_length):
             window = clean_data.iloc[i : i + window_size]
             # æª¢æŸ¥ç‰¹å¾µæ•¸é‡æ˜¯å¦ç¬¦åˆæ¨¡å‹é æœŸ (ä¾‹å¦‚ 7 å€‹ç‰¹å¾µ)
             if window.shape[1] != 7:
                 # é€™è£¡å¯ä»¥åŠ å…¥å‹•æ…‹èª¿æ•´æ¨¡å‹æˆ–å ±éŒ¯çš„é‚è¼¯
                 pass
             x_data.append(window.values)
             
        # è½‰ç‚º Tensor
        x_tensor = torch.tensor(np.array(x_data), dtype=torch.float32)
        y_tensor = torch.tensor(labels, dtype=torch.long)
        
        # åˆ†å‰²è³‡æ–™é›†
        x_train, x_temp, y_train, y_temp = train_test_split(x_tensor, y_tensor, test_size=0.2, random_state=42)
        
        # æº–å‚™ DataLoader
        batch_size = config.BATCH_SIZE
        train_dataset = TensorDataset(x_train, F.one_hot(y_train, num_classes=2).float())
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        input_size = window_size
        output_size = 2
        model = TCN(input_size, output_size)
        
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
        
        # è¨“ç·´è¿´åœˆ
        num_epochs = config.EPOCHS
        print(f"é–‹å§‹è¨“ç·´ {num_epochs} Epochs...")
        
        for epoch in range(num_epochs):
            model.train()
            running_loss = 0.0
            for inputs, targets in train_loader:
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
            
            # å®šæœŸè¼¸å‡ºè¨“ç·´ç‹€æ…‹
            if (epoch + 1) % 10 == 0:
                print(f'Stock {stock_code} | Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')
                
        print(f"âœ… æ¨¡å‹ {stock_code} è¨“ç·´å®Œæˆï¼")
        
        # å„²å­˜æ¨¡å‹
        utils.ensure_dir_exists(config.MODEL_SAVE_DIR)
        model_path = os.path.join(config.MODEL_SAVE_DIR, f'model_{stock_code}.pth')
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³: {model_path}")

    except Exception as e:
        print(f"âŒ è¨“ç·´éç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ ({stock_code}): {e}")

def main():
    target_stocks = list(config.SYMBOL_DICT.keys())
    for stock in target_stocks:
        train_model(stock)

if __name__ == "__main__":
    main()