import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# å°å…¥è‡ªå®šç¾©æ¨¡çµ„
import config
import utils

# å®šç¾© TCN æ¨¡å‹
# Input shape: (Batch, Window_Size, Features)
class TCN(nn.Module):
    def __init__(self, input_size, output_size):
        super(TCN, self).__init__()
        # input_size: è¦–çª—å¤§å° (config.WINDOW_SIZE)
        # é€™è£¡å‡è¨­ Features æ•¸é‡ç‚ºæ¨™çš„è‚¡ç¥¨æ•¸
        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=2)
        
        # å‹•æ…‹è¨ˆç®— Flatten å¾Œçš„ç¶­åº¦
        # è¼¸å…¥å½¢ç‹€: (Batch, Window_Size, Num_Features)
        # Conv1d ä½œç”¨åœ¨ Num_Features ç¶­åº¦ä¸Š
        self.fc1 = None 
        self.fc2 = nn.Linear(128, output_size)

    def forward(self, x):
        # x shape: (Batch, Window, Features)
        x = self.conv1(x) 
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1) 
        
        if self.fc1 is None:
            self.fc1 = nn.Linear(x.size(1), 128).to(x.device)
            
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

def load_and_align_data():
    """
    è¼‰å…¥åŸºé‡‘èˆ‡æ‰€æœ‰è§€å¯Ÿæ¨™çš„è‚¡ç¥¨è³‡æ–™ä¸¦é€²è¡Œæ™‚é–“å°é½Š
    """
    # è¼‰å…¥åŸºé‡‘è³‡æ–™
    fund_path = os.path.join(config.FUND_DATA_DIR, f"{config.TARGET_FUND['id']}.csv")
    if not os.path.exists(fund_path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°åŸºé‡‘è³‡æ–™: {fund_path}")
        return None
    
    fund_df = pd.read_csv(fund_path)
    fund_df = fund_df.rename(columns={'net_value': 'target_val', 'date': 'date'})
    fund_df['date'] = pd.to_datetime(fund_df['date'])
    
    merged_df = fund_df[['date', 'target_val']]
    
    # è¼‰å…¥ä¸¦åˆä½µè§€å¯Ÿæ¨™çš„è‚¡ç¥¨
    for stock_code, name in config.WATCH_STOCKS.items():
        file_name = f"{stock_code}{name}.csv"
        stock_path = os.path.join(config.STOCK_DATA_DIR, file_name)
        
        if os.path.exists(stock_path):
            stock_df = pd.read_csv(stock_path)
            # è­‰äº¤æ‰€è³‡æ–™æ¬„ä½: ['æ—¥æœŸ', 'æˆäº¤è‚¡æ•¸', ..., 'æ”¶ç›¤åƒ¹', ...]
            # ç¢ºä¿æ¬„ä½æ­£ç¢º
            col_map = {'æ—¥æœŸ': 'date', 'æ”¶ç›¤åƒ¹': f'close_{stock_code}'}
            stock_df = stock_df.rename(columns=col_map)
            stock_df['date'] = pd.to_datetime(stock_df['date'])
            
            # åªå–æ—¥æœŸèˆ‡æ”¶ç›¤åƒ¹
            stock_df = stock_df[['date', f'close_{stock_code}']]
            
            # æ¸…ç†æ”¶ç›¤åƒ¹ä¸­çš„é€—è™Ÿ
            stock_df[f'close_{stock_code}'] = stock_df[f'close_{stock_code}'].astype(str).str.replace(',', '').astype(float)
            
            merged_df = pd.merge(merged_df, stock_df, on='date', how='inner')
        else:
            print(f"âš ï¸ ç¼ºå°‘è§€å¯Ÿæ¨™çš„è³‡æ–™: {name} ({stock_code})")
            
    return merged_df.sort_values('date').reset_index(drop=True)

def preprocess_data(df):
    """
    è³‡æ–™å‰è™•ç†ï¼šæ’é™¤æ—¥æœŸï¼Œæ¨™æº–åŒ–æ‰€æœ‰ç‰¹å¾µ
    """
    features = df.drop(columns=['date', 'target_val'])
    
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)
    
    return pd.DataFrame(scaled_features, columns=features.columns), scaler

def create_labels(df, window_size):
    """
    æ ¹æ“šåŸºé‡‘æ·¨å€¼ (target_val) å»ºç«‹æ¨™ç±¤
    """
    vals = df['target_val'].values
    labels = []
    # é æ¸¬ N å¤©å¾Œçš„æ·¨å€¼æ˜¯å¦é«˜æ–¼ç•¶å‰
    for i in range(len(vals) - window_size):
        if vals[i + window_size] > vals[i]:
            labels.append(1)
        else:
            labels.append(0)
    return labels

def train_fund_model():
    """
    è¨“ç·´åŸºé‡‘é æ¸¬æ¨¡å‹
    """
    print(f"ğŸš€ é–‹å§‹è¨“ç·´åŸºé‡‘é æ¸¬æ¨¡å‹: {config.TARGET_FUND['name']}")
    
    df = load_and_align_data()
    if df is None or len(df) < 50:
        print(f"âš ï¸ è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•è¨“ç·´ã€‚ (ç›®å‰ç­†æ•¸: {len(df) if df is not None else 0})")
        return

    # å‰è™•ç†
    clean_features, scaler = preprocess_data(df)
    window_size = config.WINDOW_SIZE
    
    x_data = []
    labels = create_labels(df, window_size)
    
    for i in range(len(labels)):
        window = clean_features.iloc[i : i + window_size]
        x_data.append(window.values)
        
    x_tensor = torch.tensor(np.array(x_data), dtype=torch.float32)
    y_tensor = torch.tensor(labels, dtype=torch.long)
    
    # åˆ†å‰²è³‡æ–™
    x_train, x_test, y_train, y_test = train_test_split(x_tensor, y_tensor, test_size=0.2, random_state=42)
    
    train_dataset = TensorDataset(x_train, F.one_hot(y_train, 2).float())
    test_dataset = TensorDataset(x_test, F.one_hot(y_test, 2).float())
    
    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = TCN(window_size, 2)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    
    # è¨“ç·´
    for epoch in range(config.EPOCHS):
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # è©•ä¼°æ¨¡å¼ (Evaluation Mode)
        model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, targets in test_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                test_loss += loss.item()
                
                # è¨ˆç®—æº–ç¢ºåº¦
                _, predicted = torch.max(outputs.data, 1)
                _, labels = torch.max(targets.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        if (epoch + 1) % 10 == 0:
            avg_train_loss = train_loss / len(train_loader)
            avg_test_loss = test_loss / len(test_loader)
            accuracy = correct / total if total > 0 else 0
            print(f'Epoch [{epoch+1}/{config.EPOCHS}]')
            print(f'  Train Loss: {avg_train_loss:.4f} | Test Loss: {avg_test_loss:.4f}')
            print(f'  Test Accuracy: {accuracy:.2%}')

    # å„²å­˜
    utils.ensure_dir_exists(config.MODEL_SAVE_DIR)
    torch.save({
        'model_state': model.state_dict(),
        'scaler': scaler,
        'features': list(clean_features.columns)
    }, os.path.join(config.MODEL_SAVE_DIR, 'fund_model.pth'))
    print("âœ… åŸºé‡‘é æ¸¬æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å·²å„²å­˜ã€‚")

def main():
    train_fund_model()

if __name__ == "__main__":
    main()